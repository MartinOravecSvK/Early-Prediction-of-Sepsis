{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5046131-8973-4c66-8a2e-682324d65305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# Why not use mixture of experts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c24c5cc-49b0-40b2-a885-0c65aa274634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f6cb9b-e7cc-403a-b905-a62af86d88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heart_rate_data(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dataset['HR'].hist(bins=50)\n",
    "    plt.title('Distribution of Heart Rate')\n",
    "    plt.xlabel('Heart Rate')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # You can also get a quick statistical summary\n",
    "    print(dataset['HR'].describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb50c9a9-9558-4ae1-97a8-e622c521313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Loads the dataset\n",
    "\n",
    "# Sepsis related test values / variables / columns\n",
    "sep_col = ['BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST',\n",
    "             'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "             'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n",
    "             'Bilirubin_total', 'Hct', 'Hgb', 'PTT', 'WBC', 'Platelets',\n",
    "             'Bilirubin_direct', 'Fibrinogen']\n",
    "\n",
    "# Continues Health Indicators\n",
    "con_col = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2']\n",
    "\n",
    "# The original way of getting data shouldn't work as there isn't a concept of individual patient file in it\n",
    "# It just gets the data completely into a dataframe and each of the time data is one row\n",
    "# dataset = get_data.get_dataset_as_df()\n",
    "\n",
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf90cc6-ee19-4eae-81e9-88981a73a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for patient_id, file_name in patient_id_map.items():\n",
    "#     print(type(dataset.loc[patient_id]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4ab624-7a23-4070-b00c-cfca8ebc9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "columns_to_ffill = [\n",
    "    'Temp', 'Glucose', 'Potassium', 'Calcium', \n",
    "    'Magnesium', 'Chloride', 'Hct', 'Hgb', 'WBC', 'Platelets'\n",
    "]\n",
    "columns_to_drop = [\n",
    "    'SepsisLabel', 'TroponinI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03ff48c-1b03-45d6-8a9a-e6bffa7c7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_missing_information(patient_data, columns):\n",
    "    # temp_data holds the information from the patient file as well as the features that will be calculated\n",
    "    temp_data = np.array(patient_data)\n",
    "\n",
    "    # Calculate 3 features for each column, 2 respective of the frequency of NaN values and 1 respective of the change in recorded values\n",
    "    for column in columns:\n",
    "        data = np.array(patient_data[column])\n",
    "        nan_pos = np.where(~np.isnan(data))[0]\n",
    "        \n",
    "        # Measurement frequency sequence\n",
    "        interval_f1 = data.copy()\n",
    "        # Measurement time interval\n",
    "        interval_f2 = data.copy()\n",
    "\n",
    "        # If all the values are NaN\n",
    "        if (len(nan_pos) == 0):\n",
    "            interval_f1[:] = 0\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "            interval_f2[:] = -1\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "        else :\n",
    "            # Puts number of measurements into temp_data\n",
    "            interval_f1[: nan_pos[0]] = 0\n",
    "            for p in range(len(nan_pos)-1):\n",
    "                interval_f1[nan_pos[p]: nan_pos[p+1]] = p + 1\n",
    "            interval_f1[nan_pos[-1] :] = len(nan_pos)\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "\n",
    "            # Puts the frequency of measurements into temp_data\n",
    "            interval_f2[:nan_pos[0]] = -1\n",
    "            for q in range(len(nan_pos) - 1):\n",
    "                length = nan_pos[q+1] - nan_pos[q]\n",
    "                for l in range(length):\n",
    "                    interval_f2[nan_pos[q] + l] = l\n",
    "\n",
    "            length = len(patient_data) - nan_pos[-1]\n",
    "            for l in range(length):\n",
    "                interval_f2[nan_pos[-1] + l] = l\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "\n",
    "        # Differential features\n",
    "        # These capture the change in values that have been recorded (quite simply as well but it should be just fine)\n",
    "        diff_f = data.copy()\n",
    "        diff_f = diff_f.astype(float)\n",
    "        if len(nan_pos) <= 1:\n",
    "            diff_f[:] = np.NaN\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "        else:\n",
    "            diff_f[:nan_pos[1]] = np.NaN\n",
    "            for p in range(1, len(nan_pos)-1):\n",
    "                diff_f[nan_pos[p] : nan_pos[p+1]] = data[nan_pos[p]] - data[nan_pos[p-1]]\n",
    "            diff_f[nan_pos[-1]:] = data[nan_pos[-1]] - data[nan_pos[-2]]\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "    \n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b01ff8-e0f1-408d-8582-d1e35e53b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_slide_window(patient_data, columns):\n",
    "    \n",
    "    window_size = 6\n",
    "    features = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        series = patient_data[column]\n",
    "\n",
    "        features[f'{column}_max'] = series.rolling(window=window_size, min_periods=1).max()\n",
    "        features[f'{column}_min'] = series.rolling(window=window_size, min_periods=1).min()\n",
    "        features[f'{column}_mean'] = series.rolling(window=window_size, min_periods=1).mean()\n",
    "        features[f'{column}_median'] = series.rolling(window=window_size, min_periods=1).median()\n",
    "        features[f'{column}_std'] = series.rolling(window=window_size, min_periods=1).std()\n",
    "        \n",
    "        # For calculating std dev of differences, use diff() then apply rolling std\n",
    "        diff_std = series.diff().rolling(window=window_size, min_periods=1).std()\n",
    "        features[f'{column}_diff_std'] = diff_std\n",
    "\n",
    "    # Convert the dictionary of features into a DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3094ea18-fd79-4824-9ba9-06c5208a1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_score(patient_data):\n",
    "    \"\"\"\n",
    "    Gives score assocciated with the patient data according to the scoring systems of NEWS, SOFA and qSOFA\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = np.zeros((len(patient_data), 8))\n",
    "    \n",
    "    for ii in range(len(patient_data)):\n",
    "        HR = patient_data[ii, 0]\n",
    "        if HR == np.nan:\n",
    "            HR_score = np.nan\n",
    "        elif (HR <= 40) | (HR >= 131):\n",
    "            HR_score = 3\n",
    "        elif 111 <= HR <= 130:\n",
    "            HR_score = 2\n",
    "        elif (41 <= HR <= 50) | (91 <= HR <= 110):\n",
    "            HR_score = 1\n",
    "        else:\n",
    "            HR_score = 0\n",
    "        scores[ii, 0] = HR_score\n",
    "\n",
    "        Temp = patient_data[ii, 2]\n",
    "        if Temp == np.nan:\n",
    "            Temp_score = np.nan\n",
    "        elif Temp <= 35:\n",
    "            Temp_score = 3\n",
    "        elif Temp >= 39.1:\n",
    "            Temp_score = 2\n",
    "        elif (35.1 <= Temp <= 36.0) | (38.1 <= Temp <= 39.0):\n",
    "            Temp_score = 1\n",
    "        else:\n",
    "            Temp_score = 0\n",
    "        scores[ii, 1] = Temp_score\n",
    "\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if Resp == np.nan:\n",
    "            Resp_score = np.nan\n",
    "        elif (Resp < 8) | (Resp > 25):\n",
    "            Resp_score = 3\n",
    "        elif 21 <= Resp <= 24:\n",
    "            Resp_score = 2\n",
    "        elif 9 <= Resp <= 11:\n",
    "            Resp_score = 1\n",
    "        else:\n",
    "            Resp_score = 0\n",
    "        scores[ii, 2] = Resp_score\n",
    "\n",
    "        Creatinine = patient_data[ii, 19]\n",
    "        if Creatinine == np.nan:\n",
    "            Creatinine_score = np.nan\n",
    "        elif Creatinine < 1.2:\n",
    "            Creatinine_score = 0\n",
    "        elif Creatinine < 2:\n",
    "            Creatinine_score = 1\n",
    "        elif Creatinine < 3.5:\n",
    "            Creatinine_score = 2\n",
    "        else:\n",
    "            Creatinine_score = 3\n",
    "        scores[ii, 3] = Creatinine_score\n",
    "\n",
    "        MAP = patient_data[ii, 4]\n",
    "        if MAP == np.nan:\n",
    "            MAP_score = np.nan\n",
    "        elif MAP >= 70:\n",
    "            MAP_score = 0\n",
    "        else:\n",
    "            MAP_score = 1\n",
    "        scores[ii, 4] = MAP_score\n",
    "\n",
    "        SBP = patient_data[ii, 3]\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if SBP + Resp == np.nan:\n",
    "            qsofa = np.nan\n",
    "        elif (SBP <= 100) & (Resp >= 22):\n",
    "            qsofa = 1\n",
    "        else:\n",
    "            qsofa = 0\n",
    "        scores[ii, 5] = qsofa\n",
    "\n",
    "        Platelets = patient_data[ii, 30]\n",
    "        if Platelets == np.nan:\n",
    "            Platelets_score = np.nan\n",
    "        elif Platelets <= 50:\n",
    "            Platelets_score = 3\n",
    "        elif Platelets <= 100:\n",
    "            Platelets_score = 2\n",
    "        elif Platelets <= 150:\n",
    "            Platelets_score = 1\n",
    "        else:\n",
    "            Platelets_score = 0\n",
    "        scores[ii, 6] = Platelets_score\n",
    "\n",
    "        Bilirubin = patient_data[ii, 25]\n",
    "        if Bilirubin == np.nan:\n",
    "            Bilirubin_score = np.nan\n",
    "        elif Bilirubin < 1.2:\n",
    "            Bilirubin_score = 0\n",
    "        elif Bilirubin < 2:\n",
    "            Bilirubin_score = 1\n",
    "        elif Bilirubin < 6:\n",
    "            Bilirubin_score = 2\n",
    "        else:\n",
    "            Bilirubin_score = 3\n",
    "        scores[ii, 7] = Bilirubin_score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2975050-0b04-412e-bea6-a12702b2debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(patient_data):\n",
    "    # Get the column with Sepsis Label as it is not the same for each row (check documentation)\n",
    "    labels = np.array(patient_data['SepsisLabel'])\n",
    "    patient_data = patient_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Gets information from the missing variables \n",
    "    # This can be useful as it shows the clinical judgment, the test has not been ordered \n",
    "    #                              (probably a good decision we should take into account)\n",
    "    temp_data = feature_missing_information(patient_data, sep_col + con_col)\n",
    "    temp = pd.DataFrame(temp_data)\n",
    "    # To complete the data use forward-filling strategy\n",
    "    temp = temp.fillna(method='ffill')\n",
    "    # These are also the first set of features\n",
    "    # In this configutation 99 (66 + 33 or 3 per column) features to be precise\n",
    "    # They are also time indifferent\n",
    "    features_A = np.array(temp)\n",
    "    # The team did not use DBP, not sure why, might investigate this\n",
    "    # columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'DBP']\n",
    "    \n",
    "    # six-hour slide window statistics of selected columns\n",
    "    columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp']\n",
    "    features_B = feature_slide_window(patient_data, columns)\n",
    "\n",
    "    # Score features based according to NEWS, SOFA and qSOFA\n",
    "    features_C = features_score(features_A)\n",
    "    \n",
    "    features = np.column_stack([features_A, features_B, features_C])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b8a3b1-2dbd-4a57-b3d1-e788e2e34604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "def preprecess_data(dataset):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "    \n",
    "    for patient_id in set(dataset.index.get_level_values(0)):\n",
    "        print(f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\", end='\\r')\n",
    "        \n",
    "        patient_data = dataset.loc[patient_id]\n",
    "    \n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features)\n",
    "        labels = pd.DataFrame(labels)\n",
    "    \n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = np.array(pd.concat(frames_features))\n",
    "    data_labels = (np.array(pd.concat(frames_labels)))[:, 0]\n",
    "    \n",
    "    # Randomly shuffle the data\n",
    "    index = [i for i in range(len(data_labels))]\n",
    "    np.random.shuffle(index)\n",
    "    data_features = data_features[index]\n",
    "    data_labels = data_labels[index]\n",
    "    \n",
    "    return data_features, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "614a16bf-ac7a-4a9c-9441-7d5f85345bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocess function\n",
    "# features, labels = preprecess_data(dataset)\n",
    "# print(\"Done with data pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b4d1c7-ab6e-457f-9c02-e058c086ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8211a2f8-e7a9-47f4-923b-cc873410e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple XGBoost model training\n",
    "\n",
    "def train_model(X, y, kfold, save_dir):\n",
    "    best_f1 = -1  # Initialize best score to a low value\n",
    "    best_model = None  # Placeholder for the best model\n",
    "    \n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        # Splitting the data for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Optionally, standardize features\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Initialize and train the XGBoost model\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print()\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = model\n",
    "            \n",
    "    # Display average scores\n",
    "    print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "    # After finding the best model, save it\n",
    "    if best_model is not None:\n",
    "        save_model_path = save_dir + 'simple_xgboost_model{}.mdl'.format(1)\n",
    "        best_model.get_booster().save_model(fname=save_model_path)\n",
    "        print(f\"Best model saved with F1 score: {best_f1}\")\n",
    "    else:\n",
    "        print(\"No model was saved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60b7718-f9c3-4d90-9bb9-40da90e9c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully copied function to downsample data\n",
    "# Reason is it is quite simple and provides a great explanation as to why it is needed\n",
    "def downsample(data_set):\n",
    "    \"\"\"\n",
    "    Using our feature extraction approach will result in over 1 million hours of data in the training process.\n",
    "    However, only roughly 1.8% of these data corresponds to a positive outcome.\n",
    "    Consequently, in order to deal with the serious class imbalance, a systematic way is provided by\n",
    "    down sampling the excessive data instances of the majority class in each cross validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Runs preprocessing here (combined with loading)\n",
    "    x, y = preprecess_data(data_set)\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "\n",
    "    index = index_0[len(index_1): -1]\n",
    "    x_del = np.delete(x, index, 0)\n",
    "    y_del = np.delete(y, index, 0)\n",
    "    index = [i for i in range(len(y_del))]\n",
    "    np.random.shuffle(index)\n",
    "    x_del = x_del[index]\n",
    "    y_del = y_del[index]\n",
    "\n",
    "    return x_del, y_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e16499c-3934-4c9c-b813-6018d0f7742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for patient ID: 40336.0, File: p107128.psv\r"
     ]
    }
   ],
   "source": [
    "# Normally only ~1.8% has sepsis label 1\n",
    "# print(sum(labels)/len(labels))\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=np.random.seed(1337))\n",
    "\n",
    "save_path = '../models/saved/'\n",
    "\n",
    "X, y = downsample(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc2b976-dced-4284-ac46-05e2f5239325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average Accuracy: 0.8638\n",
      "Average F1 Score: 0.8660\n",
      "Best model saved with F1 score: 0.8674225515578965\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "train_model(X, y, kfold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "081c6a0d-5752-4b56-9f0b-4a80ebd086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for the actual model:\n",
    "\n",
    "# Bayesian Optimization with the Tree-structured Parzen Estimator (TPE) algorithm\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK\n",
    "\n",
    "def BO_TPE(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Hyperparameter optimization using Bayesian Optimization with TPE.\n",
    "    \"\"\"\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    val = xgb.DMatrix(X_val, label=y_val)\n",
    "    X_val_D = xgb.DMatrix(X_val)\n",
    "\n",
    "    def objective(params):\n",
    "        # Training and validation\n",
    "        xgb_model = xgb.train(params, dtrain=train, num_boost_round=1000, evals=[(val, 'eval')],\n",
    "                              verbose_eval=False, early_stopping_rounds=80)\n",
    "        # Prediction\n",
    "        y_vd_pred = xgb_model.predict(X_val_D)\n",
    "        y_val_class = [0 if i <= 0.5 else 1 for i in y_vd_pred]\n",
    "        # Evaluation\n",
    "        loss = 1 - accuracy_score(y_val, y_val_class)\n",
    "\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "    max_depths = [3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    learning_rates = [0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.15, 0.2]\n",
    "    subsamples = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    colsample_bytrees = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    reg_alphas = [0.0, 0.005, 0.01, 0.05, 0.1]\n",
    "    reg_lambdas = [0.8, 1, 1.5, 2, 4]\n",
    "    gammas = [0, 0.1, 0.2, 0.5, 1]\n",
    "    min_child_weights = [1, 5, 10, 20]\n",
    "    num_boost_rounds = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500]\n",
    "    \n",
    "    # Hyperparameter space\n",
    "    space = {\n",
    "        'max_depth': hp.choice('max_depth', max_depths),\n",
    "        'learning_rate': hp.choice('learning_rate', learning_rates),\n",
    "        'subsample': hp.choice('subsample', subsamples),\n",
    "        'colsample_bytree': hp.choice('colsample_bytree', colsample_bytrees),\n",
    "        'reg_alpha': hp.choice('reg_alpha', reg_alphas),\n",
    "        'reg_lambda': hp.choice('reg_lambda', reg_lambdas),\n",
    "        'gamma': hp.choice('gamma', gammas),\n",
    "        'min_child_weight': hp.choice('min_child_weight', min_child_weights),\n",
    "        'num_boost_round': hp.choice('num_boost_round', num_boost_rounds)\n",
    "    }\n",
    "\n",
    "    # Optimization\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
    "    \n",
    "    # Translate 'best' indices to actual values\n",
    "    best_param = {\n",
    "        'max_depth': max_depths[(best['max_depth'])],\n",
    "        'learning_rate': learning_rates[(best['learning_rate'])],\n",
    "        'subsample': subsamples[(best['subsample'])],\n",
    "        'colsample_bytree': colsample_bytrees[(best['colsample_bytree'])],\n",
    "        'reg_alpha': reg_alphas[(best['reg_alpha'])],\n",
    "        'reg_lambda': reg_lambdas[(best['reg_lambda'])],\n",
    "        'gamma': gammas[(best['gamma'])],\n",
    "        'min_child_weight': min_child_weights[(best['min_child_weight'])],\n",
    "        'num_boost_round': num_boost_rounds[(best['num_boost_round'])]\n",
    "    }\n",
    "    \n",
    "    return best_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "881f8311-ab6b-4ae0-a613-6a3e92281c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual model config used in the paper\n",
    "\n",
    "def train_actual_model(k, X_train, y_train, X_val, y_val, save_dir):\n",
    "    print(\"Hyperparameter optimization\")\n",
    "    # Finds the best hyperparameters\n",
    "    best_param = BO_TPE(X_train, y_train, X_val, y_val)\n",
    "    # Creates the XGBoost Classifier using the best found hyperparameters\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth = best_param['max_depth'],\n",
    "        eta = best_param['learning_rate'],\n",
    "        n_estimators = best_param['num_boost_round'],\n",
    "        subsample = best_param['subsample'],\n",
    "        colsample_bytree = best_param['colsample_bytree'],\n",
    "        reg_alpha = best_param['reg_alpha'],\n",
    "        reg_lambda = best_param['reg_lambda'],\n",
    "        gamma = best_param['gamma'],\n",
    "        min_child_weight = best_param['min_child_weight'],\n",
    "        objective = \"binary:logistic\"\n",
    "    )\n",
    "    # Fits the classifier\n",
    "    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric='error', early_stopping_rounds=80, verbose=False)\n",
    "\n",
    "    # Printing training AUC and accurace\n",
    "    y_tr_pred = (model.predict_proba(X_train))[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, y_tr_pred)\n",
    "    print('training dataset AUC: ' + str(train_auc))\n",
    "    best = 0\n",
    "    threshold = 0\n",
    "    for t in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        y_tr_class = [0 if i <= t else 1 for i in y_tr_pred]\n",
    "        acc = accuracy_score(y_train, y_tr_class)\n",
    "        if (acc > best):\n",
    "            best = acc\n",
    "            threshold = t\n",
    "    print('training dataset acc: ' + str(best) + ' using threshold of ' + str(threshold))\n",
    "\n",
    "    # Priting validation AUC and accuracy\n",
    "    y_vd_pred = (model.predict_proba(X_val))[:, 1]\n",
    "    valid_auc = roc_auc_score(y_val, y_vd_pred)\n",
    "    print('validation dataset AUC: ' + str(valid_auc))\n",
    "    best = 0\n",
    "    threshold = 0\n",
    "    for t in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        y_val_class = [0 if i <= 0.5 else 1 for i in y_vd_pred]\n",
    "        acc = accuracy_score(y_val, y_val_class)\n",
    "        if (acc > best):\n",
    "            best = acc\n",
    "            threshold = t\n",
    "    print('validation dataset acc: ' + str(best) + ' using thresholld of ' + str(threshold))\n",
    "    \n",
    "    print(f\"Using following parameters:\\n\\\n",
    "            max_depth : {best_param['max_depth']}\\n\\\n",
    "            eta (learning rate) : {best_param['learning_rate']}\\n\\\n",
    "            n_estimators : {best_param['num_boost_round']}\\n\\\n",
    "            subsample : {best_param['subsample']}\\n\\\n",
    "            colsample_bytree : {best_param['colsample_bytree']}\\n\\\n",
    "            reg_alpha : {best_param['reg_alpha']}\\n\\\n",
    "            reg_lambda ; {best_param['reg_lambda']}\\n\\\n",
    "            gamma : {best_param['gamma']}\\n\\\n",
    "            min_child_weight : {best_param['min_child_weight']}\\n\\\n",
    "            objective : 'binary:logistic'\")\n",
    "    \n",
    "    save_model_path = save_dir + 'xgboost_model{}.mdl'.format(k + 1)\n",
    "    model.get_booster().save_model(fname=save_model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97af9b30-ed8c-4e09-afe2-142e2e5748fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for patient ID: 40336.0, File: p107128.psv\r"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "# We might do this each loop iteration\n",
    "# Also consider changing the downsample function slightly \n",
    "X, y = downsample(dataset)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=np.random.seed(1337))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "351eed85-950a-4d40-a7aa-a98480c847c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization\n",
      "100%|████████████████████████████████████████████| 20/20 [08:15<00:00, 24.76s/trial, best loss: 0.07092325602220828]\n",
      "training dataset AUC: 0.9969720618313376\n",
      "training dataset acc: 0.9733801997044732 using threshold of 0.5\n",
      "validation dataset AUC: 0.9549961329098515\n",
      "validation dataset acc: 0.8920032237843647 using thresholld of 0.1\n",
      "Using following parameters:\n",
      "            max_depth : 8\n",
      "            eta (learning rate) : 0.2\n",
      "            n_estimators : 100\n",
      "            subsample : 0.9\n",
      "            colsample_bytree : 0.5\n",
      "            reg_alpha : 0.01\n",
      "            reg_lambda ; 1\n",
      "            gamma : 0\n",
      "            min_child_weight : 1\n",
      "            objective : 'binary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|████████████████████████████████████████████| 20/20 [12:09<00:00, 36.49s/trial, best loss: 0.06116235336258624]\n",
      "training dataset AUC: 0.999988028368968\n",
      "training dataset acc: 0.9980522097344736 using threshold of 0.5\n",
      "validation dataset AUC: 0.9815645608453695\n",
      "validation dataset acc: 0.9352556640100296 using thresholld of 0.1\n",
      "Using following parameters:\n",
      "            max_depth : 11\n",
      "            eta (learning rate) : 0.04\n",
      "            n_estimators : 800\n",
      "            subsample : 0.9\n",
      "            colsample_bytree : 0.6\n",
      "            reg_alpha : 0.05\n",
      "            reg_lambda ; 1\n",
      "            gamma : 0\n",
      "            min_child_weight : 1\n",
      "            objective : 'binary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|████████████████████████████████████████████| 20/20 [04:22<00:00, 13.10s/trial, best loss: 0.08193785260141484]\n",
      "training dataset AUC: 0.9998321085990665\n",
      "training dataset acc: 0.9949626113822594 using threshold of 0.5\n",
      "validation dataset AUC: 0.9684754893205787\n",
      "validation dataset acc: 0.9142115160741471 using thresholld of 0.1\n",
      "Using following parameters:\n",
      "            max_depth : 10\n",
      "            eta (learning rate) : 0.08\n",
      "            n_estimators : 1100\n",
      "            subsample : 0.6\n",
      "            colsample_bytree : 0.7\n",
      "            reg_alpha : 0.005\n",
      "            reg_lambda ; 0.8\n",
      "            gamma : 0.1\n",
      "            min_child_weight : 20\n",
      "            objective : 'binary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|████████████████████████████████████████████| 20/20 [05:53<00:00, 17.67s/trial, best loss: 0.06716818914562062]\n",
      "training dataset AUC: 0.9978047237161075\n",
      "training dataset acc: 0.9774553921239394 using threshold of 0.5\n",
      "validation dataset AUC: 0.9589317533226952\n",
      "validation dataset acc: 0.8987999283539316 using thresholld of 0.1\n",
      "Using following parameters:\n",
      "            max_depth : 10\n",
      "            eta (learning rate) : 0.04\n",
      "            n_estimators : 400\n",
      "            subsample : 0.5\n",
      "            colsample_bytree : 0.8\n",
      "            reg_alpha : 0.01\n",
      "            reg_lambda ; 2\n",
      "            gamma : 0\n",
      "            min_child_weight : 5\n",
      "            objective : 'binary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|████████████████████████████████████████████| 20/20 [05:46<00:00, 17.33s/trial, best loss: 0.07576571735626003]\n",
      "training dataset AUC: 0.9939149393457273\n",
      "training dataset acc: 0.9605749210826785 using threshold of 0.5\n",
      "validation dataset AUC: 0.946435002985343\n",
      "validation dataset acc: 0.8800823929786853 using thresholld of 0.1\n",
      "Using following parameters:\n",
      "            max_depth : 10\n",
      "            eta (learning rate) : 0.08\n",
      "            n_estimators : 200\n",
      "            subsample : 0.5\n",
      "            colsample_bytree : 0.9\n",
      "            reg_alpha : 0.1\n",
      "            reg_lambda ; 0.8\n",
      "            gamma : 0\n",
      "            min_child_weight : 10\n",
      "            objective : 'binary:logistic'\n"
     ]
    }
   ],
   "source": [
    "# Run training loop\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(X)):\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "    train_actual_model(fold, X_train, y_train, X_val, y_val, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36a2e207-8e45-4ee6-b576-151a353f0d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNotes:\\n- Can't really use mean values to substitute for NaN values since the values have increased probability of being unhealthy since the test was ordered by a doctor\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notes:\n",
    "- Can't really use mean values to substitute for NaN values since the values have increased probability of being unhealthy since the test was ordered by a doctor\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4ae16fb-617a-4c9e-9dd6-61032fd35f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a5b6bb-2a3f-4b9b-a8b5-2ab8cb003021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesTransformer1(nn.Module):\n",
    "    def __init__(self, num_features, num_blocks=1, d_model=64, nhead=4, dim_feedforward=256, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)\n",
    "        self.output_layer = nn.Linear(d_model, 1)  # Outputting a probability for each time step\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)  # (Batch, Seq, Features) -> (Batch, Seq, d_model)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.output_layer(output)\n",
    "        return torch.sigmoid(output)  # Apply sigmoid to get probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef9f7a6c-d1cc-4881-92f4-a98735ae346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ed3c4aa-0123-480c-b5e1-a7d7bd860166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly interpolating:\n",
      "HR\n",
      "O2Sat\n",
      "SBP\n",
      "MAP\n",
      "DBP\n",
      "Resp\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# First lets experiment with only raw data \n",
    "# We have to however impute NaN values since Neural Networks can't (natively) handle them\n",
    "\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "\n",
    "# Feel free to omit this (EXPERIMENTAL)\n",
    "# Normilize the dataset\n",
    "if True:\n",
    "    # Check if multiindex_df is indeed a MultiIndex DataFrame\n",
    "    if isinstance(dataset.index, pd.MultiIndex):\n",
    "        # Normalize each patient's data\n",
    "        # This will apply z-score normalization per patient per feature\n",
    "        normalized_data = dataset.groupby(level=0).transform(lambda x: (x - x.mean()) / x.std())\n",
    "    \n",
    "        # Optionally fill NaN values if they are created by division by zero in cases where std is zero\n",
    "        normalized_data = normalized_data.fillna(0)\n",
    "    \n",
    "        # If you need to replace the old DataFrame with the new, normalized one\n",
    "        dataset.update(normalized_data)\n",
    "    else:\n",
    "        print(\"The dataframe does not have a MultiIndex as expected.\")\n",
    "\n",
    "    \n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    dataset = impute_linear_interpolation(dataset, col)\n",
    "    print(col)\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13cfc8-2791-4e47-b247-f2a0761a4e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.05951389279103011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.057526902803787174\n",
      "Epoch 2, Loss: 0.05738054602967546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.057276453830434615\n",
      "Epoch 3, Loss: 0.056865705219966936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.05747072567494781\n",
      "Epoch 4, Loss: 0.056626043007487574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.056420056654861835\n",
      "Epoch 5, Loss: 0.05643747643437209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.05624296069827712\n",
      "Epoch 6, Loss: 0.056289507521573974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.05640595956457657\n",
      "Epoch 7, Loss: 0.056162805683411754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=40, out_features=64, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.056663643000764534\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming 'dataset' has been appropriately preprocessed and split\n",
    "features = dataset.drop('SepsisLabel', axis=1).values\n",
    "labels = dataset['SepsisLabel'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).clamp(0, 1) \n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).clamp(0, 1)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "num_features = X_train.shape[1]\n",
    "model = TimeSeriesTransformer(num_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = []\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output.squeeze(), target)\n",
    "            valid_loss.append(loss.item())\n",
    "        print(f'Validation Loss: {np.mean(valid_loss)}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'time_series_transformer_model.pth')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e22b81-61d1-4fa1-932a-cf2f090beafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e229d-4b09-4ef2-8ad2-0d1cdee90e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_blocks=6, d_model=128, nhead=8, dim_feedforward=512, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        # Embedding layer expanded to take original features + missingness indicators\n",
    "        self.embedding = nn.Linear(num_features * 2, d_model)  \n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, d_model))  # Assuming max sequence length of 1000\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src)\n",
    "        seq_length = src.size(1)\n",
    "        src += self.positional_encoding[:, :seq_length]\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.output_layer(output)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6bfb1-1fc8-4f5a-aaca-948ea3a4529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training.\")\n",
    "\n",
    "# Prepare data and add missingness indicators\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val = add_nan_indicators(X_train), add_nan_indicators(X_val)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = TimeSeriesTransformer(num_features=X_train.shape[1] // 2)  # Assuming features were doubled to account for indicators\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# To keep all the data focus on the minority class (sepsis = 1)\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCELoss(weight=class_weights[1])  # Focus more on the minority class\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 30\n",
    "best_auroc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            preds = output.squeeze()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(target.tolist())\n",
    "\n",
    "    # Calculate AUROC\n",
    "    auroc = roc_auc_score(all_targets, all_preds)\n",
    "    if auroc > best_auroc:\n",
    "        best_auroc = auroc\n",
    "        # Save model and predictions for the best model based on AUROC\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Validation AUROC: {auroc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
