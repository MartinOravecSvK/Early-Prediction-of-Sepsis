{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5046131-8973-4c66-8a2e-682324d65305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# Why not use mixture of experts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c24c5cc-49b0-40b2-a885-0c65aa274634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f6cb9b-e7cc-403a-b905-a62af86d88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heart_rate_data(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dataset['HR'].hist(bins=50)\n",
    "    plt.title('Distribution of Heart Rate')\n",
    "    plt.xlabel('Heart Rate')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # You can also get a quick statistical summary\n",
    "    print(dataset['HR'].describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb50c9a9-9558-4ae1-97a8-e622c521313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Loads the dataset\n",
    "\n",
    "# Sepsis related test values / variables / columns\n",
    "sep_col = ['BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST',\n",
    "             'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "             'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n",
    "             'Bilirubin_total', 'Hct', 'Hgb', 'PTT', 'WBC', 'Platelets',\n",
    "             'Bilirubin_direct', 'Fibrinogen']\n",
    "\n",
    "# Continues Health Indicators\n",
    "con_col = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2']\n",
    "\n",
    "# The original way of getting data shouldn't work as there isn't a concept of individual patient file in it\n",
    "# It just gets the data completely into a dataframe and each of the time data is one row\n",
    "# dataset = get_data.get_dataset_as_df()\n",
    "\n",
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf90cc6-ee19-4eae-81e9-88981a73a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for patient_id, file_name in patient_id_map.items():\n",
    "#     print(type(dataset.loc[patient_id]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4ab624-7a23-4070-b00c-cfca8ebc9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "columns_to_ffill = [\n",
    "    'Temp', 'Glucose', 'Potassium', 'Calcium', \n",
    "    'Magnesium', 'Chloride', 'Hct', 'Hgb', 'WBC', 'Platelets'\n",
    "]\n",
    "columns_to_drop = [\n",
    "    'SepsisLabel', 'TroponinI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03ff48c-1b03-45d6-8a9a-e6bffa7c7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_missing_information(patient_data, columns):\n",
    "    # temp_data holds the information from the patient file as well as the features that will be calculated\n",
    "    temp_data = np.array(patient_data)\n",
    "\n",
    "    # Calculate 3 features for each column, 2 respective of the frequency of NaN values and 1 respective of the change in recorded values\n",
    "    for column in columns:\n",
    "        data = np.array(patient_data[column])\n",
    "        nan_pos = np.where(~np.isnan(data))[0]\n",
    "        \n",
    "        # Measurement frequency sequence\n",
    "        interval_f1 = data.copy()\n",
    "        # Measurement time interval\n",
    "        interval_f2 = data.copy()\n",
    "\n",
    "        # If all the values are NaN\n",
    "        if (len(nan_pos) == 0):\n",
    "            interval_f1[:] = 0\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "            interval_f2[:] = -1\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "        else :\n",
    "            # Puts number of measurements into temp_data\n",
    "            interval_f1[: nan_pos[0]] = 0\n",
    "            for p in range(len(nan_pos)-1):\n",
    "                interval_f1[nan_pos[p]: nan_pos[p+1]] = p + 1\n",
    "            interval_f1[nan_pos[-1] :] = len(nan_pos)\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "\n",
    "            # Puts the frequency of measurements into temp_data\n",
    "            interval_f2[:nan_pos[0]] = -1\n",
    "            for q in range(len(nan_pos) - 1):\n",
    "                length = nan_pos[q+1] - nan_pos[q]\n",
    "                for l in range(length):\n",
    "                    interval_f2[nan_pos[q] + l] = l\n",
    "\n",
    "            length = len(patient_data) - nan_pos[-1]\n",
    "            for l in range(length):\n",
    "                interval_f2[nan_pos[-1] + l] = l\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "\n",
    "        # Differential features\n",
    "        # These capture the change in values that have been recorded (quite simply as well but it should be just fine)\n",
    "        diff_f = data.copy()\n",
    "        diff_f = diff_f.astype(float)\n",
    "        if len(nan_pos) <= 1:\n",
    "            diff_f[:] = np.NaN\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "        else:\n",
    "            diff_f[:nan_pos[1]] = np.NaN\n",
    "            for p in range(1, len(nan_pos)-1):\n",
    "                diff_f[nan_pos[p] : nan_pos[p+1]] = data[nan_pos[p]] - data[nan_pos[p-1]]\n",
    "            diff_f[nan_pos[-1]:] = data[nan_pos[-1]] - data[nan_pos[-2]]\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "    \n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b01ff8-e0f1-408d-8582-d1e35e53b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_slide_window(patient_data, columns):\n",
    "    \n",
    "    window_size = 6\n",
    "    features = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        series = patient_data[column]\n",
    "\n",
    "        features[f'{column}_max'] = series.rolling(window=window_size, min_periods=1).max()\n",
    "        features[f'{column}_min'] = series.rolling(window=window_size, min_periods=1).min()\n",
    "        features[f'{column}_mean'] = series.rolling(window=window_size, min_periods=1).mean()\n",
    "        features[f'{column}_median'] = series.rolling(window=window_size, min_periods=1).median()\n",
    "        features[f'{column}_std'] = series.rolling(window=window_size, min_periods=1).std()\n",
    "        \n",
    "        # For calculating std dev of differences, use diff() then apply rolling std\n",
    "        diff_std = series.diff().rolling(window=window_size, min_periods=1).std()\n",
    "        features[f'{column}_diff_std'] = diff_std\n",
    "\n",
    "    # Convert the dictionary of features into a DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3094ea18-fd79-4824-9ba9-06c5208a1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_score(patient_data):\n",
    "    \"\"\"\n",
    "    Gives score assocciated with the patient data according to the scoring systems of NEWS, SOFA and qSOFA\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = np.zeros((len(patient_data), 8))\n",
    "    \n",
    "    for ii in range(len(patient_data)):\n",
    "        HR = patient_data[ii, 0]\n",
    "        if HR == np.nan:\n",
    "            HR_score = np.nan\n",
    "        elif (HR <= 40) | (HR >= 131):\n",
    "            HR_score = 3\n",
    "        elif 111 <= HR <= 130:\n",
    "            HR_score = 2\n",
    "        elif (41 <= HR <= 50) | (91 <= HR <= 110):\n",
    "            HR_score = 1\n",
    "        else:\n",
    "            HR_score = 0\n",
    "        scores[ii, 0] = HR_score\n",
    "\n",
    "        Temp = patient_data[ii, 2]\n",
    "        if Temp == np.nan:\n",
    "            Temp_score = np.nan\n",
    "        elif Temp <= 35:\n",
    "            Temp_score = 3\n",
    "        elif Temp >= 39.1:\n",
    "            Temp_score = 2\n",
    "        elif (35.1 <= Temp <= 36.0) | (38.1 <= Temp <= 39.0):\n",
    "            Temp_score = 1\n",
    "        else:\n",
    "            Temp_score = 0\n",
    "        scores[ii, 1] = Temp_score\n",
    "\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if Resp == np.nan:\n",
    "            Resp_score = np.nan\n",
    "        elif (Resp < 8) | (Resp > 25):\n",
    "            Resp_score = 3\n",
    "        elif 21 <= Resp <= 24:\n",
    "            Resp_score = 2\n",
    "        elif 9 <= Resp <= 11:\n",
    "            Resp_score = 1\n",
    "        else:\n",
    "            Resp_score = 0\n",
    "        scores[ii, 2] = Resp_score\n",
    "\n",
    "        Creatinine = patient_data[ii, 19]\n",
    "        if Creatinine == np.nan:\n",
    "            Creatinine_score = np.nan\n",
    "        elif Creatinine < 1.2:\n",
    "            Creatinine_score = 0\n",
    "        elif Creatinine < 2:\n",
    "            Creatinine_score = 1\n",
    "        elif Creatinine < 3.5:\n",
    "            Creatinine_score = 2\n",
    "        else:\n",
    "            Creatinine_score = 3\n",
    "        scores[ii, 3] = Creatinine_score\n",
    "\n",
    "        MAP = patient_data[ii, 4]\n",
    "        if MAP == np.nan:\n",
    "            MAP_score = np.nan\n",
    "        elif MAP >= 70:\n",
    "            MAP_score = 0\n",
    "        else:\n",
    "            MAP_score = 1\n",
    "        scores[ii, 4] = MAP_score\n",
    "\n",
    "        SBP = patient_data[ii, 3]\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if SBP + Resp == np.nan:\n",
    "            qsofa = np.nan\n",
    "        elif (SBP <= 100) & (Resp >= 22):\n",
    "            qsofa = 1\n",
    "        else:\n",
    "            qsofa = 0\n",
    "        scores[ii, 5] = qsofa\n",
    "\n",
    "        Platelets = patient_data[ii, 30]\n",
    "        if Platelets == np.nan:\n",
    "            Platelets_score = np.nan\n",
    "        elif Platelets <= 50:\n",
    "            Platelets_score = 3\n",
    "        elif Platelets <= 100:\n",
    "            Platelets_score = 2\n",
    "        elif Platelets <= 150:\n",
    "            Platelets_score = 1\n",
    "        else:\n",
    "            Platelets_score = 0\n",
    "        scores[ii, 6] = Platelets_score\n",
    "\n",
    "        Bilirubin = patient_data[ii, 25]\n",
    "        if Bilirubin == np.nan:\n",
    "            Bilirubin_score = np.nan\n",
    "        elif Bilirubin < 1.2:\n",
    "            Bilirubin_score = 0\n",
    "        elif Bilirubin < 2:\n",
    "            Bilirubin_score = 1\n",
    "        elif Bilirubin < 6:\n",
    "            Bilirubin_score = 2\n",
    "        else:\n",
    "            Bilirubin_score = 3\n",
    "        scores[ii, 7] = Bilirubin_score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2975050-0b04-412e-bea6-a12702b2debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(patient_data):\n",
    "    # Get the column with Sepsis Label as it is not the same for each row (check documentation)\n",
    "    labels = np.array(patient_data['SepsisLabel'])\n",
    "    patient_data = patient_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Gets information from the missing variables \n",
    "    # This can be useful as it shows the clinical judgment, the test has not been ordered \n",
    "    #                              (probably a good decision we should take into account)\n",
    "    temp_data = feature_missing_information(patient_data, sep_col + con_col)\n",
    "    temp = pd.DataFrame(temp_data)\n",
    "    # To complete the data use forward-filling strategy\n",
    "    temp = temp.fillna(method='ffill')\n",
    "    # These are also the first set of features\n",
    "    # In this configutation 99 (66 + 33 or 3 per column) features to be precise\n",
    "    # They are also time indifferent\n",
    "    features_A = np.array(temp)\n",
    "    # The team did not use DBP, not sure why, might investigate this\n",
    "    # columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'DBP']\n",
    "    \n",
    "    # six-hour slide window statistics of selected columns\n",
    "    columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp']\n",
    "    features_B = feature_slide_window(patient_data, columns)\n",
    "\n",
    "    # Score features based according to NEWS, SOFA and qSOFA\n",
    "    features_C = features_score(features_A)\n",
    "    \n",
    "    features = np.column_stack([features_A, features_B, features_C])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b8a3b1-2dbd-4a57-b3d1-e788e2e34604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "def preprecess_data(dataset):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "    \n",
    "    for patient_id in set(dataset.index.get_level_values(0)):\n",
    "        print(f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\", end='\\r')\n",
    "        \n",
    "        patient_data = dataset.loc[patient_id]\n",
    "    \n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features)\n",
    "        labels = pd.DataFrame(labels)\n",
    "    \n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = np.array(pd.concat(frames_features))\n",
    "    data_labels = (np.array(pd.concat(frames_labels)))[:, 0]\n",
    "    \n",
    "    # Randomly shuffle the data\n",
    "    index = [i for i in range(len(data_labels))]\n",
    "    np.random.shuffle(index)\n",
    "    data_features = data_features[index]\n",
    "    data_labels = data_labels[index]\n",
    "    \n",
    "    return data_features, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "614a16bf-ac7a-4a9c-9441-7d5f85345bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with data pre-processingD: 40336.0, File: p107128.psv\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocess function\n",
    "# features, labels = preprecess_data(dataset)\n",
    "# print(\"Done with data pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84b4d1c7-ab6e-457f-9c02-e058c086ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8211a2f8-e7a9-47f4-923b-cc873410e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple XGBoost model training\n",
    "\n",
    "def train_model(X, y, kfold, save_dir):\n",
    "    best_f1 = -1  # Initialize best score to a low value\n",
    "    best_model = None  # Placeholder for the best model\n",
    "    \n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        # Splitting the data for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Optionally, standardize features\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Initialize and train the XGBoost model\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print()\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = model\n",
    "            \n",
    "    # Display average scores\n",
    "    print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "    # After finding the best model, save it\n",
    "    if best_model is not None:\n",
    "        save_model_path = save_dir + 'simple_xgboost_model{}.mdl'.format(1)\n",
    "        best_model.get_booster().save_model(fname=save_model_path)\n",
    "        print(f\"Best model saved with F1 score: {best_f1}\")\n",
    "    else:\n",
    "        print(\"No model was saved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b60b7718-f9c3-4d90-9bb9-40da90e9c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully copied function to downsample data\n",
    "# Reason is it is quite simple and provides a great explanation as to why it is needed\n",
    "def downsample(data_set):\n",
    "    \"\"\"\n",
    "    Using our feature extraction approach will result in over 1 million hours of data in the training process.\n",
    "    However, only roughly 1.8% of these data corresponds to a positive outcome.\n",
    "    Consequently, in order to deal with the serious class imbalance, a systematic way is provided by\n",
    "    down sampling the excessive data instances of the majority class in each cross validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Runs preprocessing here (combined with loading)\n",
    "    x, y = preprecess_data(data_set)\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "\n",
    "    index = index_0[len(index_1): -1]\n",
    "    x_del = np.delete(x, index, 0)\n",
    "    y_del = np.delete(y, index, 0)\n",
    "    index = [i for i in range(len(y_del))]\n",
    "    np.random.shuffle(index)\n",
    "    x_del = x_del[index]\n",
    "    y_del = y_del[index]\n",
    "\n",
    "    return x_del, y_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e16499c-3934-4c9c-b813-6018d0f7742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for patient ID: 40336.0, File: p107128.psv\r"
     ]
    }
   ],
   "source": [
    "# Normally only ~1.8% has sepsis label 1\n",
    "# print(sum(labels)/len(labels))\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=np.random.seed(1337))\n",
    "\n",
    "save_path = '../models/saved/'\n",
    "\n",
    "X, y = downsample(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbc2b976-dced-4284-ac46-05e2f5239325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best model saved with F1 score: 0.8717768195018921\n",
      "Average Accuracy: 0.8634\n",
      "Average F1 Score: 0.8658\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "train_model(X, y, kfold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfb1530-8fc5-4878-b7ee-312ee37f4c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished imputing HR\n",
      "Finished imputing O2Sat\n",
      "Finished imputing SBP\n",
      "Finished imputing MAP\n",
      "Finished imputing DBP\n",
      "Finished imputing Resp\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "# Imputes O2Sat using linear interpolation\n",
    "# Other methods might be better based on the data distribution (consider Spline or Polynomial Interpolation)\n",
    "\n",
    "# Impute SBP using linear interpolation\n",
    "# We can consider Forward Fill or Backward Fill if we assume the blood pressure should remain relatively stable\n",
    "\n",
    "# Impute MAP using linear interpolation\n",
    "# To be more sophiscticated the data can be imputed with custom models to take into account SBP and DBP as there might be correlation\n",
    "\n",
    "# Impute DBP using linear interpolation\n",
    "# Same as SBP, we might consider Spline\n",
    "\n",
    "# Impute Resp using linear interpolation\n",
    "# Same as SBP and DBP, we might consider Spline or Polynomial Interpolation\n",
    "\n",
    "\n",
    "'''\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "for column in columns_to_linearly_interpolate:\n",
    "    dataset = impute_linear_interpolation(dataset, column)\n",
    "    print('Finished imputing ' + column)\n",
    "\n",
    "columns_to_ffill = [\n",
    "    'Temp', 'Glucose', 'Potassium', 'Calcium', \n",
    "    'Magnesium', 'Chloride', 'Hct', 'Hgb', 'WBC', 'Platelets'\n",
    "]\n",
    "for column in columns_to_ffill:\n",
    "    dataset[column].ffill(inplace=True)\n",
    "'''\n",
    "\n",
    "# Columns not imputed\n",
    "\n",
    "# They dropped Bilirubin_direct, TroponinI, Fibrinogen\n",
    "#            has relation, more complex if any, potentially\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset = dataset.fillna\n",
    "\n",
    "# Use forward filling for some of the data\n",
    "\n",
    "# Best solution used sliding window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
