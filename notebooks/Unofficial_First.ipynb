{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5046131-8973-4c66-8a2e-682324d65305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# Why not use mixture of experts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c24c5cc-49b0-40b2-a885-0c65aa274634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f6cb9b-e7cc-403a-b905-a62af86d88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heart_rate_data(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dataset['HR'].hist(bins=50)\n",
    "    plt.title('Distribution of Heart Rate')\n",
    "    plt.xlabel('Heart Rate')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # You can also get a quick statistical summary\n",
    "    print(dataset['HR'].describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb50c9a9-9558-4ae1-97a8-e622c521313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Loads the dataset\n",
    "\n",
    "# Sepsis related test values / variables / columns\n",
    "sep_col = ['BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST',\n",
    "             'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "             'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n",
    "             'Bilirubin_total', 'Hct', 'Hgb', 'PTT', 'WBC', 'Platelets',\n",
    "             'Bilirubin_direct', 'Fibrinogen']\n",
    "\n",
    "# Continues Health Indicators\n",
    "con_col = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2']\n",
    "\n",
    "# The original way of getting data shouldn't work as there isn't a concept of individual patient file in it\n",
    "# It just gets the data completely into a dataframe and each of the time data is one row\n",
    "# dataset = get_data.get_dataset_as_df()\n",
    "\n",
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf90cc6-ee19-4eae-81e9-88981a73a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for patient_id, file_name in patient_id_map.items():\n",
    "#     print(type(dataset.loc[patient_id]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4ab624-7a23-4070-b00c-cfca8ebc9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "columns_to_ffill = [\n",
    "    'Temp', 'Glucose', 'Potassium', 'Calcium', \n",
    "    'Magnesium', 'Chloride', 'Hct', 'Hgb', 'WBC', 'Platelets'\n",
    "]\n",
    "columns_to_drop = [\n",
    "    'SepsisLabel', 'TroponinI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03ff48c-1b03-45d6-8a9a-e6bffa7c7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_missing_information(patient_data, columns):\n",
    "    # temp_data holds the information from the patient file as well as the features that will be calculated\n",
    "    temp_data = np.array(patient_data)\n",
    "\n",
    "    # Calculate 3 features for each column, 2 respective of the frequency of NaN values and 1 respective of the change in recorded values\n",
    "    for column in columns:\n",
    "        data = np.array(patient_data[column])\n",
    "        nan_pos = np.where(~np.isnan(data))[0]\n",
    "        \n",
    "        # Measurement frequency sequence\n",
    "        interval_f1 = data.copy()\n",
    "        # Measurement time interval\n",
    "        interval_f2 = data.copy()\n",
    "\n",
    "        # If all the values are NaN\n",
    "        if (len(nan_pos) == 0):\n",
    "            interval_f1[:] = 0\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "            interval_f2[:] = -1\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "        else :\n",
    "            # Puts number of measurements into temp_data\n",
    "            interval_f1[: nan_pos[0]] = 0\n",
    "            for p in range(len(nan_pos)-1):\n",
    "                interval_f1[nan_pos[p]: nan_pos[p+1]] = p + 1\n",
    "            interval_f1[nan_pos[-1] :] = len(nan_pos)\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "\n",
    "            # Puts the frequency of measurements into temp_data\n",
    "            interval_f2[:nan_pos[0]] = -1\n",
    "            for q in range(len(nan_pos) - 1):\n",
    "                length = nan_pos[q+1] - nan_pos[q]\n",
    "                for l in range(length):\n",
    "                    interval_f2[nan_pos[q] + l] = l\n",
    "\n",
    "            length = len(patient_data) - nan_pos[-1]\n",
    "            for l in range(length):\n",
    "                interval_f2[nan_pos[-1] + l] = l\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "\n",
    "        # Differential features\n",
    "        # These capture the change in values that have been recorded (quite simply as well but it should be just fine)\n",
    "        diff_f = data.copy()\n",
    "        diff_f = diff_f.astype(float)\n",
    "        if len(nan_pos) <= 1:\n",
    "            diff_f[:] = np.NaN\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "        else:\n",
    "            diff_f[:nan_pos[1]] = np.NaN\n",
    "            for p in range(1, len(nan_pos)-1):\n",
    "                diff_f[nan_pos[p] : nan_pos[p+1]] = data[nan_pos[p]] - data[nan_pos[p-1]]\n",
    "            diff_f[nan_pos[-1]:] = data[nan_pos[-1]] - data[nan_pos[-2]]\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "    \n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b01ff8-e0f1-408d-8582-d1e35e53b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_slide_window(patient_data, columns):\n",
    "    \n",
    "    window_size = 6\n",
    "    features = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        series = patient_data[column]\n",
    "\n",
    "        features[f'{column}_max'] = series.rolling(window=window_size, min_periods=1).max()\n",
    "        features[f'{column}_min'] = series.rolling(window=window_size, min_periods=1).min()\n",
    "        features[f'{column}_mean'] = series.rolling(window=window_size, min_periods=1).mean()\n",
    "        features[f'{column}_median'] = series.rolling(window=window_size, min_periods=1).median()\n",
    "        features[f'{column}_std'] = series.rolling(window=window_size, min_periods=1).std()\n",
    "        \n",
    "        # For calculating std dev of differences, use diff() then apply rolling std\n",
    "        diff_std = series.diff().rolling(window=window_size, min_periods=1).std()\n",
    "        features[f'{column}_diff_std'] = diff_std\n",
    "\n",
    "    # Convert the dictionary of features into a DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3094ea18-fd79-4824-9ba9-06c5208a1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_score(patient_data):\n",
    "    \"\"\"\n",
    "    Gives score assocciated with the patient data according to the scoring systems of NEWS, SOFA and qSOFA\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = np.zeros((len(patient_data), 8))\n",
    "    \n",
    "    for ii in range(len(patient_data)):\n",
    "        HR = patient_data[ii, 0]\n",
    "        if HR == np.nan:\n",
    "            HR_score = np.nan\n",
    "        elif (HR <= 40) | (HR >= 131):\n",
    "            HR_score = 3\n",
    "        elif 111 <= HR <= 130:\n",
    "            HR_score = 2\n",
    "        elif (41 <= HR <= 50) | (91 <= HR <= 110):\n",
    "            HR_score = 1\n",
    "        else:\n",
    "            HR_score = 0\n",
    "        scores[ii, 0] = HR_score\n",
    "\n",
    "        Temp = patient_data[ii, 2]\n",
    "        if Temp == np.nan:\n",
    "            Temp_score = np.nan\n",
    "        elif Temp <= 35:\n",
    "            Temp_score = 3\n",
    "        elif Temp >= 39.1:\n",
    "            Temp_score = 2\n",
    "        elif (35.1 <= Temp <= 36.0) | (38.1 <= Temp <= 39.0):\n",
    "            Temp_score = 1\n",
    "        else:\n",
    "            Temp_score = 0\n",
    "        scores[ii, 1] = Temp_score\n",
    "\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if Resp == np.nan:\n",
    "            Resp_score = np.nan\n",
    "        elif (Resp < 8) | (Resp > 25):\n",
    "            Resp_score = 3\n",
    "        elif 21 <= Resp <= 24:\n",
    "            Resp_score = 2\n",
    "        elif 9 <= Resp <= 11:\n",
    "            Resp_score = 1\n",
    "        else:\n",
    "            Resp_score = 0\n",
    "        scores[ii, 2] = Resp_score\n",
    "\n",
    "        Creatinine = patient_data[ii, 19]\n",
    "        if Creatinine == np.nan:\n",
    "            Creatinine_score = np.nan\n",
    "        elif Creatinine < 1.2:\n",
    "            Creatinine_score = 0\n",
    "        elif Creatinine < 2:\n",
    "            Creatinine_score = 1\n",
    "        elif Creatinine < 3.5:\n",
    "            Creatinine_score = 2\n",
    "        else:\n",
    "            Creatinine_score = 3\n",
    "        scores[ii, 3] = Creatinine_score\n",
    "\n",
    "        MAP = patient_data[ii, 4]\n",
    "        if MAP == np.nan:\n",
    "            MAP_score = np.nan\n",
    "        elif MAP >= 70:\n",
    "            MAP_score = 0\n",
    "        else:\n",
    "            MAP_score = 1\n",
    "        scores[ii, 4] = MAP_score\n",
    "\n",
    "        SBP = patient_data[ii, 3]\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if SBP + Resp == np.nan:\n",
    "            qsofa = np.nan\n",
    "        elif (SBP <= 100) & (Resp >= 22):\n",
    "            qsofa = 1\n",
    "        else:\n",
    "            qsofa = 0\n",
    "        scores[ii, 5] = qsofa\n",
    "\n",
    "        Platelets = patient_data[ii, 30]\n",
    "        if Platelets == np.nan:\n",
    "            Platelets_score = np.nan\n",
    "        elif Platelets <= 50:\n",
    "            Platelets_score = 3\n",
    "        elif Platelets <= 100:\n",
    "            Platelets_score = 2\n",
    "        elif Platelets <= 150:\n",
    "            Platelets_score = 1\n",
    "        else:\n",
    "            Platelets_score = 0\n",
    "        scores[ii, 6] = Platelets_score\n",
    "\n",
    "        Bilirubin = patient_data[ii, 25]\n",
    "        if Bilirubin == np.nan:\n",
    "            Bilirubin_score = np.nan\n",
    "        elif Bilirubin < 1.2:\n",
    "            Bilirubin_score = 0\n",
    "        elif Bilirubin < 2:\n",
    "            Bilirubin_score = 1\n",
    "        elif Bilirubin < 6:\n",
    "            Bilirubin_score = 2\n",
    "        else:\n",
    "            Bilirubin_score = 3\n",
    "        scores[ii, 7] = Bilirubin_score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2975050-0b04-412e-bea6-a12702b2debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(patient_data):\n",
    "    # Get the column with Sepsis Label as it is not the same for each row (check documentation)\n",
    "    labels = np.array(patient_data['SepsisLabel'])\n",
    "    patient_data = patient_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Gets information from the missing variables \n",
    "    # This can be useful as it shows the clinical judgment, the test has not been ordered \n",
    "    #                              (probably a good decision we should take into account)\n",
    "    temp_data = feature_missing_information(patient_data, sep_col + con_col)\n",
    "    temp = pd.DataFrame(temp_data)\n",
    "    # To complete the data use forward-filling strategy\n",
    "    temp = temp.fillna(method='ffill')\n",
    "    # These are also the first set of features\n",
    "    # In this configutation 99 (66 + 33 or 3 per column) features to be precise\n",
    "    # They are also time indifferent\n",
    "    features_A = np.array(temp)\n",
    "    # The team did not use DBP, not sure why, might investigate this\n",
    "    # columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'DBP']\n",
    "    \n",
    "    # six-hour slide window statistics of selected columns\n",
    "    columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp']\n",
    "    features_B = feature_slide_window(patient_data, columns)\n",
    "\n",
    "    # Score features based according to NEWS, SOFA and qSOFA\n",
    "    features_C = features_score(features_A)\n",
    "    \n",
    "    features = np.column_stack([features_A, features_B, features_C])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b8a3b1-2dbd-4a57-b3d1-e788e2e34604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "def preprecess_data(dataset):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "    \n",
    "    for patient_id in set(dataset.index.get_level_values(0)):\n",
    "        print(f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\", end='\\r')\n",
    "        \n",
    "        patient_data = dataset.loc[patient_id]\n",
    "    \n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features)\n",
    "        labels = pd.DataFrame(labels)\n",
    "    \n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = np.array(pd.concat(frames_features))\n",
    "    data_labels = (np.array(pd.concat(frames_labels)))[:, 0]\n",
    "    \n",
    "    # Randomly shuffle the data\n",
    "    index = [i for i in range(len(data_labels))]\n",
    "    np.random.shuffle(index)\n",
    "    data_features = data_features[index]\n",
    "    data_labels = data_labels[index]\n",
    "    \n",
    "    return data_features, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "614a16bf-ac7a-4a9c-9441-7d5f85345bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocess function\n",
    "# features, labels = preprecess_data(dataset)\n",
    "# print(\"Done with data pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b4d1c7-ab6e-457f-9c02-e058c086ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8211a2f8-e7a9-47f4-923b-cc873410e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple XGBoost model training\n",
    "\n",
    "def train_model(X, y, kfold, save_dir):\n",
    "    best_f1 = -1  # Initialize best score to a low value\n",
    "    best_model = None  # Placeholder for the best model\n",
    "    \n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        # Splitting the data for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Optionally, standardize features\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Initialize and train the XGBoost model\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print()\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = model\n",
    "            \n",
    "    # Display average scores\n",
    "    print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "    # After finding the best model, save it\n",
    "    if best_model is not None:\n",
    "        save_model_path = save_dir + 'simple_xgboost_model{}.mdl'.format(1)\n",
    "        best_model.get_booster().save_model(fname=save_model_path)\n",
    "        print(f\"Best model saved with F1 score: {best_f1}\")\n",
    "    else:\n",
    "        print(\"No model was saved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60b7718-f9c3-4d90-9bb9-40da90e9c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully copied function to downsample data\n",
    "# Reason is it is quite simple and provides a great explanation as to why it is needed\n",
    "def downsample(data_set):\n",
    "    \"\"\"\n",
    "    Using our feature extraction approach will result in over 1 million hours of data in the training process.\n",
    "    However, only roughly 1.8% of these data corresponds to a positive outcome.\n",
    "    Consequently, in order to deal with the serious class imbalance, a systematic way is provided by\n",
    "    down sampling the excessive data instances of the majority class in each cross validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Runs preprocessing here (combined with loading)\n",
    "    x, y = preprecess_data(data_set)\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "\n",
    "    index = index_0[len(index_1): -1]\n",
    "    x_del = np.delete(x, index, 0)\n",
    "    y_del = np.delete(y, index, 0)\n",
    "    index = [i for i in range(len(y_del))]\n",
    "    np.random.shuffle(index)\n",
    "    x_del = x_del[index]\n",
    "    y_del = y_del[index]\n",
    "\n",
    "    return x_del, y_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e16499c-3934-4c9c-b813-6018d0f7742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for patient ID: 40336.0, File: p107128.psv\r"
     ]
    }
   ],
   "source": [
    "# Normally only ~1.8% has sepsis label 1\n",
    "# print(sum(labels)/len(labels))\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=np.random.seed(1337))\n",
    "\n",
    "save_path = '../models/saved/'\n",
    "\n",
    "X, y = downsample(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc2b976-dced-4284-ac46-05e2f5239325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average Accuracy: 0.8638\n",
      "Average F1 Score: 0.8660\n",
      "Best model saved with F1 score: 0.8674225515578965\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "train_model(X, y, kfold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "081c6a0d-5752-4b56-9f0b-4a80ebd086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for the actual model:\n",
    "\n",
    "# Bayesian Optimization with the Tree-structured Parzen Estimator (TPE) algorithm\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK\n",
    "\n",
    "def BO_TPE(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Hyperparameter optimization using Bayesian Optimization with TPE.\n",
    "    \"\"\"\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    val = xgb.DMatrix(X_val, label=y_val)\n",
    "    X_val_D = xgb.DMatrix(X_val)\n",
    "\n",
    "    def objective(params):\n",
    "        # Training and validation\n",
    "        xgb_model = xgb.train(params, dtrain=train, num_boost_round=1000, evals=[(val, 'eval')],\n",
    "                              verbose_eval=False, early_stopping_rounds=80)\n",
    "        # Prediction\n",
    "        y_vd_pred = xgb_model.predict(X_val_D)\n",
    "        y_val_class = [0 if i <= 0.5 else 1 for i in y_vd_pred]\n",
    "        # Evaluation\n",
    "        loss = 1 - accuracy_score(y_val, y_val_class)\n",
    "\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "    max_depths = [3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    learning_rates = [0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.15, 0.2]\n",
    "    subsamples = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    colsample_bytrees = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    reg_alphas = [0.0, 0.005, 0.01, 0.05, 0.1]\n",
    "    reg_lambdas = [0.8, 1, 1.5, 2, 4]\n",
    "    gammas = [0, 0.1, 0.2, 0.5, 1]\n",
    "    min_child_weights = [1, 5, 10, 20]\n",
    "    num_boost_rounds = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500]\n",
    "    \n",
    "    # Hyperparameter space\n",
    "    space = {\n",
    "        'max_depth': hp.choice('max_depth', max_depths),\n",
    "        'learning_rate': hp.choice('learning_rate', learning_rates),\n",
    "        'subsample': hp.choice('subsample', subsamples),\n",
    "        'colsample_bytree': hp.choice('colsample_bytree', colsample_bytrees),\n",
    "        'reg_alpha': hp.choice('reg_alpha', reg_alphas),\n",
    "        'reg_lambda': hp.choice('reg_lambda', reg_lambdas),\n",
    "        'gamma': hp.choice('gamma', gammas),\n",
    "        'min_child_weight': hp.choice('min_child_weight', min_child_weights),\n",
    "        'num_boost_round': hp.choice('num_boost_round', num_boost_rounds)\n",
    "    }\n",
    "\n",
    "    # Optimization\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
    "    \n",
    "    # Translate 'best' indices to actual values\n",
    "    best_param = {\n",
    "        'max_depth': max_depths[(best['max_depth'])],\n",
    "        'learning_rate': learning_rates[(best['learning_rate'])],\n",
    "        'subsample': subsamples[(best['subsample'])],\n",
    "        'colsample_bytree': colsample_bytrees[(best['colsample_bytree'])],\n",
    "        'reg_alpha': reg_alphas[(best['reg_alpha'])],\n",
    "        'reg_lambda': reg_lambdas[(best['reg_lambda'])],\n",
    "        'gamma': gammas[(best['gamma'])],\n",
    "        'min_child_weight': min_child_weights[(best['min_child_weight'])],\n",
    "        'num_boost_round': num_boost_rounds[(best['num_boost_round'])]\n",
    "    }\n",
    "    \n",
    "    return best_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "881f8311-ab6b-4ae0-a613-6a3e92281c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual model config used in the paper\n",
    "\n",
    "def train_actual_model(k, X_train, y_train, X_val, y_val, save_dir):\n",
    "    print(\"Hyperparameter optimization\")\n",
    "    # Finds the best hyperparameters\n",
    "    best_param = BO_TPE(X_train, y_train, X_val, y_val)\n",
    "    # Creates the XGBoost Classifier using the best found hyperparameters\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth = best_param['max_depth'],\n",
    "        eta = best_param['learning_rate'],\n",
    "        n_estimators = best_param['num_boost_round'],\n",
    "        subsample = best_param['subsample'],\n",
    "        colsample_bytree = best_param['colsample_bytree'],\n",
    "        reg_alpha = best_param['reg_alpha'],\n",
    "        reg_lambda = best_param['reg_lambda'],\n",
    "        gamma = best_param['gamma'],\n",
    "        min_child_weight = best_param['min_child_weight'],\n",
    "        objective = \"binary:logistic\"\n",
    "    )\n",
    "    # Fits the classifier\n",
    "    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric='error', early_stopping_rounds=80, verbose=False)\n",
    "\n",
    "    # Printing training AUC and accurace\n",
    "    y_tr_pred = (model.predict_proba(X_train))[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, y_tr_pred)\n",
    "    print('training dataset AUC: ' + str(train_auc))\n",
    "    y_tr_class = [0 if i <= 0.5 else 1 for i in y_tr_pred]\n",
    "    acc = accuracy_score(y_train, y_tr_class)\n",
    "    print('training dataset acc: ' + str(acc))\n",
    "\n",
    "    # Priting validation AUC and accuracy\n",
    "    y_vd_pred = (model.predict_proba(X_val))[:, 1]\n",
    "    valid_auc = roc_auc_score(y_val, y_vd_pred)\n",
    "    print('validation dataset AUC: ' + str(valid_auc))\n",
    "    y_val_class = [0 if i <= 0.5 else 1 for i in y_vd_pred]\n",
    "    acc = accuracy_score(y_val, y_val_class)\n",
    "    print('validation dataset acc: ' + str(acc))\n",
    "    \n",
    "    print(f\"Using following parameters:\\n\\\n",
    "            max_depth : {best_param['max_depth']}\\n\\\n",
    "            eta (learning rate) : {best_param['learning_rate']}\\n\\\n",
    "            n_estimators : {best_param['num_boost_round']}\\n\\\n",
    "            subsample : {best_param['subsample']}\\n\\\n",
    "            colsample_bytree : {best_param['colsample_bytree']}\\n\\\n",
    "            reg_alpha : {best_param['reg_alpha']}\\n\\\n",
    "            reg_lambda ; {best_param['reg_lambda']}\\n\\\n",
    "            gamma : {best_param['gamma']}\\n\\\n",
    "            min_child_weight : {best_param['min_child_weight']}\\n\\\n",
    "            objective : '\\binary:logistic'\")\n",
    "    \n",
    "    save_model_path = save_dir + 'xgboost_model{}.mdl'.format(k + 1)\n",
    "    model.get_booster().save_model(fname=save_model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97af9b30-ed8c-4e09-afe2-142e2e5748fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55833, 176)ata for patient ID: 40336.0, File: p107128.psv\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "# We might do this each loop iteration\n",
    "# Also consider changing the downsample function slightly \n",
    "X, y = downsample(dataset)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=np.random.seed(1337))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "351eed85-950a-4d40-a7aa-a98480c847c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization\n",
      "100%|███████████████████████████████████████████| 20/20 [03:11<00:00,  9.59s/trial, best loss: 0.06949046297125461]\n",
      "training dataset AUC: 0.9998948988546961\n",
      "training dataset acc: 0.9956566515918148\n",
      "validation dataset AUC: 0.9706548770954685\n",
      "validation dataset acc: 0.9218232291573386\n",
      "Using following parameters:\n",
      "            max_depth : 9\n",
      "            eta (learning rate) : 0.15\n",
      "            n_estimators : 700\n",
      "            subsample : 0.9\n",
      "            colsample_bytree : 0.9\n",
      "            reg_alpha : 0.01\n",
      "            reg_lambda ; 1\n",
      "            gamma : 0\n",
      "            min_child_weight : 20\n",
      "            objective : inary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|███████████████████████████████████████████| 20/20 [07:14<00:00, 21.72s/trial, best loss: 0.07226649950747743]\n",
      "training dataset AUC: 0.9943993093091128\n",
      "training dataset acc: 0.962477051896297\n",
      "validation dataset AUC: 0.9462326841101095\n",
      "validation dataset acc: 0.8798244828512581\n",
      "Using following parameters:\n",
      "            max_depth : 10\n",
      "            eta (learning rate) : 0.04\n",
      "            n_estimators : 200\n",
      "            subsample : 0.5\n",
      "            colsample_bytree : 0.5\n",
      "            reg_alpha : 0.005\n",
      "            reg_lambda ; 2\n",
      "            gamma : 0\n",
      "            min_child_weight : 1\n",
      "            objective : inary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|███████████████████████████████████████████| 20/20 [03:26<00:00, 10.35s/trial, best loss: 0.07244559863884659]\n",
      "training dataset AUC: 0.9986963093773727\n",
      "training dataset acc: 0.9833430349706712\n",
      "validation dataset AUC: 0.9646249278679679\n",
      "validation dataset acc: 0.9014059281812483\n",
      "Using following parameters:\n",
      "            max_depth : 9\n",
      "            eta (learning rate) : 0.15\n",
      "            n_estimators : 100\n",
      "            subsample : 0.9\n",
      "            colsample_bytree : 0.9\n",
      "            reg_alpha : 0.05\n",
      "            reg_lambda ; 1.5\n",
      "            gamma : 0\n",
      "            min_child_weight : 1\n",
      "            objective : inary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|███████████████████████████████████████████| 20/20 [03:16<00:00,  9.82s/trial, best loss: 0.07110872290883041]\n",
      "training dataset AUC: 0.9999325741168059\n",
      "training dataset acc: 0.9964179371795733\n",
      "validation dataset AUC: 0.9717826416711797\n",
      "validation dataset acc: 0.9205624216371127\n",
      "Using following parameters:\n",
      "            max_depth : 9\n",
      "            eta (learning rate) : 0.08\n",
      "            n_estimators : 1100\n",
      "            subsample : 0.5\n",
      "            colsample_bytree : 0.9\n",
      "            reg_alpha : 0.05\n",
      "            reg_lambda ; 0.8\n",
      "            gamma : 0\n",
      "            min_child_weight : 10\n",
      "            objective : inary:logistic'\n",
      "Hyperparameter optimization\n",
      "100%|███████████████████████████████████████████| 20/20 [04:47<00:00, 14.36s/trial, best loss: 0.07728819631022743]\n",
      "training dataset AUC: 0.9828245876502744\n",
      "training dataset acc: 0.9314930485593391\n",
      "validation dataset AUC: 0.9361981646757912\n",
      "validation dataset acc: 0.8607379545047465\n",
      "Using following parameters:\n",
      "            max_depth : 8\n",
      "            eta (learning rate) : 0.06\n",
      "            n_estimators : 300\n",
      "            subsample : 0.5\n",
      "            colsample_bytree : 0.5\n",
      "            reg_alpha : 0.005\n",
      "            reg_lambda ; 2\n",
      "            gamma : 0\n",
      "            min_child_weight : 10\n",
      "            objective : inary:logistic'\n"
     ]
    }
   ],
   "source": [
    "# Run training loop\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(X)):\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "    train_actual_model(fold, X_train, y_train, X_val, y_val, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfb1530-8fc5-4878-b7ee-312ee37f4c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished imputing HR\n",
      "Finished imputing O2Sat\n",
      "Finished imputing SBP\n",
      "Finished imputing MAP\n",
      "Finished imputing DBP\n",
      "Finished imputing Resp\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "# Imputes O2Sat using linear interpolation\n",
    "# Other methods might be better based on the data distribution (consider Spline or Polynomial Interpolation)\n",
    "\n",
    "# Impute SBP using linear interpolation\n",
    "# We can consider Forward Fill or Backward Fill if we assume the blood pressure should remain relatively stable\n",
    "\n",
    "# Impute MAP using linear interpolation\n",
    "# To be more sophiscticated the data can be imputed with custom models to take into account SBP and DBP as there might be correlation\n",
    "\n",
    "# Impute DBP using linear interpolation\n",
    "# Same as SBP, we might consider Spline\n",
    "\n",
    "# Impute Resp using linear interpolation\n",
    "# Same as SBP and DBP, we might consider Spline or Polynomial Interpolation\n",
    "\n",
    "\n",
    "'''\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "for column in columns_to_linearly_interpolate:\n",
    "    dataset = impute_linear_interpolation(dataset, column)\n",
    "    print('Finished imputing ' + column)\n",
    "\n",
    "columns_to_ffill = [\n",
    "    'Temp', 'Glucose', 'Potassium', 'Calcium', \n",
    "    'Magnesium', 'Chloride', 'Hct', 'Hgb', 'WBC', 'Platelets'\n",
    "]\n",
    "for column in columns_to_ffill:\n",
    "    dataset[column].ffill(inplace=True)\n",
    "'''\n",
    "\n",
    "# Columns not imputed\n",
    "\n",
    "# They dropped Bilirubin_direct, TroponinI, Fibrinogen\n",
    "#            has relation, more complex if any, potentially\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset = dataset.fillna\n",
    "\n",
    "# Use forward filling for some of the data\n",
    "\n",
    "# Best solution used sliding window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
