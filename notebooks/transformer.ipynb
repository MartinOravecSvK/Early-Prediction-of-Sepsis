{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f72d151-7591-4d00-a5d8-b985986461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28da82b0-0031-4338-ac71-e7b6526ab95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71f3c06-6c85-4923-98e7-18855bec62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x.transpose(0, 1))\n",
    "        x = self.decoder(x.transpose(0, 1))\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08007f84-8b79-4f55-950d-c21a3b7b6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x.transpose(0, 1))\n",
    "        x = self.decoder(x)\n",
    "        return x.squeeze(-1)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45768cab-ba20-48d5-bfdd-d37ea7863a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b726056a-3f24-44eb-ac28-5f03223f2f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1552210, 41)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb84bc8-7790-436e-a7b6-308831910611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly interpolating:\n",
      "HR\n",
      "O2Sat\n",
      "SBP\n",
      "MAP\n",
      "DBP\n",
      "Resp\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# First lets experiment with only raw data \n",
    "# We have to however impute NaN values since Neural Networks can't (natively) handle them\n",
    "\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "\n",
    "# Feel free to omit this (EXPERIMENTAL)\n",
    "# Normilize the dataset\n",
    "if True:\n",
    "    # Check if multiindex_df is indeed a MultiIndex DataFrame\n",
    "    if isinstance(dataset.index, pd.MultiIndex):\n",
    "        # Exclude 'SepsisLabel' from normalization\n",
    "        features_to_normalize = dataset.columns.difference(['SepsisLabel'])\n",
    "\n",
    "        # Normalize each patient's data\n",
    "        # This will apply z-score normalization per patient per feature, excluding 'SepsisLabel'\n",
    "        normalized_data = dataset[features_to_normalize].groupby(level=0).transform(\n",
    "            lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "        # Optionally fill NaN values if they are created by division by zero in cases where std is zero\n",
    "        normalized_data = normalized_data.fillna(0)\n",
    "\n",
    "        # Merge normalized data with the 'SepsisLabel' column\n",
    "        dataset = pd.concat([normalized_data, dataset['SepsisLabel']], axis=1)\n",
    "    else:\n",
    "        print(\"The dataframe does not have a MultiIndex as expected.\")\n",
    "\n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    if col != 'SepsisLabel':  # Ensure we do not interpolate 'SepsisLabel'\n",
    "        dataset = impute_linear_interpolation(dataset, col)\n",
    "        print(col)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f1c375-50f3-410b-9383-053786f17938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1552210, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc98495b-d998-4edc-a705-ca09320c5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_indicators(df):\n",
    "    for column in df.columns:\n",
    "        df[column + '_nan'] = df[column].isna().astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf039a4-0611-405e-9d40-19cbe40df7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(X, y):\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "    print(index_0, index_1)\n",
    "\n",
    "    if len(index_0) > len(index_1):\n",
    "        index_0 = np.random.choice(index_0, size=len(index_1), replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([index_0, index_1])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    x_balanced = X.iloc[balanced_indices]\n",
    "    y_balanced = y.iloc[balanced_indices]\n",
    "\n",
    "    return x_balanced, y_balanced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a1f7eec6-1195-4a0e-a36d-c87b7ed63995",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TimeSeriesTransformer(input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "model = TimeSeriesTransformer(input_dim=X_train.shape[1])\n",
    "summary(model, input_size=(1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119f4523-880b-4695-bc66-9b8e43a8d2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for training.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfacbf6-610c-4827-8466-666feeaac34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(28, 80)\n",
      "(34, 80)\n",
      "(37, 80)\n",
      "(29, 80)\n",
      "(47, 80)\n",
      "(26, 80)\n",
      "(44, 80)\n",
      "(56, 80)\n",
      "(37, 80)\n",
      "(32, 80)\n",
      "(26, 80)\n",
      "(27, 80)\n"
     ]
    }
   ],
   "source": [
    "X = dataset.drop('SepsisLabel', axis=1)\n",
    "X = add_nan_indicators(X)\n",
    "y = dataset['SepsisLabel']\n",
    "\n",
    "for patient_id in X.index.get_level_values('patient_id').unique():\n",
    "    print(patient_id)\n",
    "    break\n",
    "\n",
    "a = 0\n",
    "for patient_id, patient_data in X.groupby(level='patient_id'):\n",
    "    print(patient_data.shape)\n",
    "    if a > 10:\n",
    "        break\n",
    "    a += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e78a8c1-8850-4904-89f1-4fbb3ecae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_patient_data(patient_data, max_length):\n",
    "    # Standardizing the data\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(patient_data)\n",
    "    # Padding\n",
    "    padded_features = np.zeros((max_length, features.shape[1]))\n",
    "    sequence_length = min(max_length, features.shape[0])\n",
    "    padded_features[:sequence_length] = features[:sequence_length]\n",
    "    return torch.tensor(padded_features, dtype=torch.float32), sequence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85d25a30-8c8e-4850-a7aa-c481ed78c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1552210, 40)\n",
      "Max length (inputs will be padded to):  336\n",
      "457.0 336\n",
      "1126.0 336\n",
      "7228.0 336\n",
      "12849.0 336\n",
      "14201.0 336\n",
      "23823.0 336\n",
      "25080.0 336\n",
      "26436.0 336\n",
      "33001.0 336\n",
      "36236.0 336\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('SepsisLabel', axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "max_length = X.groupby('patient_id').size().max()\n",
    "print(\"Max length (inputs will be padded to): \", max_length)\n",
    "\n",
    "patient_ids = X.index.get_level_values('patient_id').unique()\n",
    "for patient_id in patient_ids:\n",
    "    max_length_patient = X.loc[patient_id].shape[0]\n",
    "    if max_length_patient == max_length:\n",
    "        print(patient_id, max_length_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50455d9a-b9bd-4027-8646-dcec78b6fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming TransformerTimeSeries and other necessary classes/functions are defined\n",
    "\n",
    "df = dataset\n",
    "X = df.drop('SepsisLabel', axis=1)\n",
    "X = add_nan_indicators(X)\n",
    "y = df['SepsisLabel']\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "max_length = X.groupby('patient_id').size().max()\n",
    "print(\"Max length (inputs will be padded to): \", max_length)\n",
    "\n",
    "patient_ids = X.index.get_level_values('patient_id').unique()\n",
    "\n",
    "# KFold configuration\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize class weights\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y.to_numpy())\n",
    "class_weights_tensor = torch.tensor(class_weights[1], dtype=torch.float)\n",
    "\n",
    "# Training loop with K-Fold Cross-Validation\n",
    "num_epochs = 10\n",
    "fold_idx = 1\n",
    "\n",
    "for train_index, val_index in kf.split(patient_ids):\n",
    "    train_ids, val_ids = patient_ids[train_index], patient_ids[val_index]\n",
    "\n",
    "    # Initialize model, criterion, and optimizer for each fold\n",
    "    input_dim = X.shape[1]\n",
    "    model = TransformerTimeSeries(input_dim=input_dim)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(f\"Starting training for fold {fold_idx}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        train_preds, train_targets = [], []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, patient_id in enumerate(train_ids):\n",
    "            start_batch = time.time()\n",
    "            \n",
    "            try:\n",
    "                patient_data = X.loc[patient_id]\n",
    "                X_train, sequence_length = prepare_patient_data(patient_data, max_length)\n",
    "                y_train = torch.tensor(y.loc[patient_id].values, dtype=torch.float32)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_train.unsqueeze(0))\n",
    "                loss = criterion(outputs[:sequence_length].squeeze(), y_train[:sequence_length])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                predicted_labels = torch.round(torch.sigmoid(outputs[:sequence_length].squeeze()))\n",
    "                train_correct += (predicted_labels == y_train[:sequence_length]).sum().item()\n",
    "                train_total += sequence_length\n",
    "\n",
    "                train_preds.extend(predicted_labels.tolist())\n",
    "                train_targets.extend(y_train[:sequence_length].tolist())\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing patient ID {patient_id}: {e}\")\n",
    "            \n",
    "            end_batch = time.time()\n",
    "            elapsed_time = end_batch - start_time\n",
    "            batches_done = i + 1\n",
    "            total_batches = len(train_ids)\n",
    "            time_per_batch = elapsed_time / batches_done\n",
    "            estimated_time_remaining = (total_batches - batches_done) * time_per_batch\n",
    "            \n",
    "            print(f'Processed {batches_done}/{total_batches} patients ({100.0 * batches_done / total_batches:.2f}%), ' +\n",
    "                  f'Estimated time remaining: {estimated_time_remaining / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "        print()  # Ensure newline after progress updates\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_precision = precision_score(train_targets, train_preds)\n",
    "        train_recall = recall_score(train_targets, train_preds)\n",
    "        train_f1 = f1_score(train_targets, train_preds)\n",
    "        print(f'Epoch {epoch+1}, Fold {fold_idx}, Avg Training Loss: {train_loss / len(train_ids)}, Training Metrics: Acc: {train_accuracy}, Precision: {train_precision}, Recall: {train_recall}, F1: {train_f1}')\n",
    "\n",
    "    # Validation after all epochs in a fold\n",
    "    validate_model(model, val_ids, X, y, max_length, criterion, fold_idx)\n",
    "    fold_idx += 1\n",
    "\n",
    "def validate_model(model, val_ids, X, y, max_length, criterion, fold_idx):\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    val_preds, val_targets = [], []\n",
    "    start_val_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, patient_id in enumerate(val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ee60a-8631-44bb-80fa-bc7ac44381bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length (inputs will be padded to):  336\n",
      "tensor(27.8014)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeries(\n",
       "  (encoder): Linear(in_features=80, out_features=64, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Training Loss: 1.2415898634351488, Training Metrics: Acc: 0.8017604933439951, Precision: 0.018793903703289472, Recall: 0.19465359768322565, F1: 0.0342782271668092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeries(\n",
       "  (encoder): Linear(in_features=80, out_features=64, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Training Loss: 1.2415898634351488, Training Metrics: Acc: 0.8017604933439951, Precision: 0.018793903703289472, Recall: 0.19465359768322565, F1: 0.0342782271668092\n",
      "Avg Validation Loss: 1.1857613008774088, Validation Metrics: Acc: 0.7753053328135722, Precision: 0.026631315363087348, Recall: 0.33046974958874065, F1: 0.04929049494963265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeries(\n",
       "  (encoder): Linear(in_features=80, out_features=64, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Training Loss: 1.2137052648617304, Training Metrics: Acc: 0.8638713942781953, Precision: 0.02369178585120831, Recall: 0.16244152372466028, F1: 0.04135240277194933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeries(\n",
       "  (encoder): Linear(in_features=80, out_features=64, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Avg Training Loss: 1.2137052648617304, Training Metrics: Acc: 0.8638713942781953, Precision: 0.02369178585120831, Recall: 0.16244152372466028, F1: 0.04135240277194933\n",
      "Avg Validation Loss: 1.2089351851439558, Validation Metrics: Acc: 0.5860521457865522, Precision: 0.019175447922203808, Recall: 0.44836410162675927, F1: 0.03677799017954196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeries(\n",
       "  (encoder): Linear(in_features=80, out_features=64, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Training Loss: 1.1536127951813997, Training Metrics: Acc: 0.8839928137212506, Precision: 0.03455956645540553, Recall: 0.20115838716863443, F1: 0.058985296134928054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeries(\n",
       "  (encoder): Linear(in_features=80, out_features=64, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Avg Training Loss: 1.1536127951813997, Training Metrics: Acc: 0.8839928137212506, Precision: 0.03455956645540553, Recall: 0.20115838716863443, F1: 0.058985296134928054\n",
      "Avg Validation Loss: 1.1453850496713631, Validation Metrics: Acc: 0.939635759135822, Precision: 0.058623902049507584, Recall: 0.16103089014805338, F1: 0.08595541245914436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeries(\n",
       "  (encoder): Linear(in_features=80, out_features=64, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 18908/32268 patients (58.60%), Estimated time remaining: 10.37 minutes\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "df = dataset\n",
    "X = df.drop('SepsisLabel', axis=1)\n",
    "X = add_nan_indicators(X)\n",
    "y = df['SepsisLabel']\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "# Yes it's really high, 336, consider making it larger to accommodate actual test set\n",
    "max_length = X.groupby('patient_id').size().max()\n",
    "print(\"Max length (inputs will be padded to): \", max_length)\n",
    "\n",
    "patient_ids = X.index.get_level_values('patient_id').unique()\n",
    "train_ids, val_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = X.shape[1]\n",
    "model = TransformerTimeSeries(input_dim=input_dim)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y.to_numpy())\n",
    "class_weights_tensor = torch.tensor(class_weights[1], dtype=torch.float)  # Weight for the positive class\n",
    "\n",
    "# Initialize BCEWithLogitsLoss with pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    train_preds, train_targets = [], []\n",
    "    \n",
    "    start_time = time.time()  # Start time for the epoch\n",
    "    \n",
    "    for i, patient_id in enumerate(train_ids):\n",
    "        start_batch = time.time()  # Start time for the batch\n",
    "        \n",
    "        try:\n",
    "            patient_data = X.loc[patient_id]\n",
    "            X_train, sequence_length = prepare_patient_data(patient_data, max_length)\n",
    "            y_train = torch.tensor(y.loc[patient_id].values, dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train.unsqueeze(0))\n",
    "            loss = criterion(outputs[:sequence_length].squeeze(), y_train[:sequence_length])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted_labels = torch.round(torch.sigmoid(outputs[:sequence_length].squeeze()))\n",
    "            train_correct += (predicted_labels == y_train[:sequence_length]).sum().item()\n",
    "            train_total += sequence_length\n",
    "\n",
    "            train_preds.extend(predicted_labels.tolist())\n",
    "            train_targets.extend(y_train[:sequence_length].tolist())\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing patient ID {patient_id}: {e}\")\n",
    "        \n",
    "        # Progress and time estimation\n",
    "        end_batch = time.time()\n",
    "        elapsed_time = end_batch - start_time\n",
    "        batches_done = i + 1\n",
    "        total_batches = len(train_ids)\n",
    "        time_per_batch = elapsed_time / batches_done\n",
    "        estimated_time_remaining = (total_batches - batches_done) * time_per_batch\n",
    "        \n",
    "        print(f'Processed {batches_done}/{total_batches} patients ({100.0 * batches_done / total_batches:.2f}%), ' +\n",
    "              f'Estimated time remaining: {estimated_time_remaining / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_precision = precision_score(train_targets, train_preds)\n",
    "    train_recall = recall_score(train_targets, train_preds)\n",
    "    train_f1 = f1_score(train_targets, train_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Avg Training Loss: {train_loss / len(train_ids)}, Training Metrics: Acc: {train_accuracy}, Precision: {train_precision}, Recall: {train_recall}, F1: {train_f1}')\n",
    "\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    val_preds, val_targets = [], []\n",
    "    start_val_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, patient_id in enumerate(val_ids):\n",
    "            start_batch = time.time()\n",
    "\n",
    "            try:\n",
    "                patient_data = X.loc[patient_id]\n",
    "                X_val, sequence_length = prepare_patient_data(patient_data, max_length)\n",
    "                y_val = torch.tensor(y.loc[patient_id].values, dtype=torch.float32)\n",
    "\n",
    "                val_outputs = model(X_val.unsqueeze(0))\n",
    "                v_loss = criterion(val_outputs[:sequence_length].squeeze(), y_val[:sequence_length])\n",
    "                val_loss += v_loss.item()\n",
    "\n",
    "                val_predicted_labels = torch.round(torch.sigmoid(val_outputs[:sequence_length].squeeze()))\n",
    "                val_correct += (val_predicted_labels == y_val[:sequence_length]).sum().item()\n",
    "                val_total += sequence_length\n",
    "\n",
    "                val_preds.extend(val_predicted_labels.tolist())\n",
    "                val_targets.extend(y_val[:sequence_length].tolist())\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing patient ID {patient_id}: {e}\")\n",
    "\n",
    "            # Progress and time estimation for validation\n",
    "            end_batch = time.time()\n",
    "            elapsed_time = end_batch - start_val_time\n",
    "            batches_done = i + 1\n",
    "            total_batches = len(val_ids)\n",
    "            time_per_batch = elapsed_time / batches_done\n",
    "            estimated_time_remaining = (total_batches - batches_done) * time_per_batch\n",
    "            \n",
    "            print(f'Validation: Processed {batches_done}/{total_batches} patients ({100.0 * batches_done / total_batches:.2f}%), ' +\n",
    "                  f'Estimated time remaining: {estimated_time_remaining / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_precision = precision_score(val_targets, val_preds)\n",
    "    val_recall = recall_score(val_targets, val_preds)\n",
    "    val_f1 = f1_score(val_targets, val_preds)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}, Avg Training Loss: {train_loss / len(train_ids)}, Training Metrics: Acc: {train_accuracy}, Precision: {train_precision}, Recall: {train_recall}, F1: {train_f1}')\n",
    "    print(f'Avg Validation Loss: {val_loss / len(val_ids)}, Validation Metrics: Acc: {val_accuracy}, Precision: {val_precision}, Recall: {val_recall}, F1: {val_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fd2f0-399d-4536-af1c-159995d3a050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
