{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f72d151-7591-4d00-a5d8-b985986461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da82b0-0031-4338-ac71-e7b6526ab95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10823102-ef8b-4c0d-9fbc-d2d861a1499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_blocks=1, d_model=64, nhead=4, dim_feedforward=128, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src)\n",
    "        # Assuming src is now [batch_size, d_model], without a seq_length\n",
    "        src = src.unsqueeze(1)  # Add a pseudo-sequence dimension\n",
    "        batch_size, seq_length, embedding_size = src.size()\n",
    "    \n",
    "        # Generate positional encodings dynamically based on sequence length and batch size\n",
    "        positional_encoding = torch.zeros(batch_size, seq_length, embedding_size).to(src.device)\n",
    "        position = torch.arange(seq_length, dtype=torch.float).unsqueeze(0).unsqueeze(-1).to(src.device)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-math.log(10000.0) / embedding_size)).to(src.device)\n",
    "        positional_encoding[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        src += positional_encoding\n",
    "    \n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.output_layer(output)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb84bc8-7790-436e-a7b6-308831910611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets experiment with only raw data \n",
    "# We have to however impute NaN values since Neural Networks can't (natively) handle them\n",
    "\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "\n",
    "# Feel free to omit this (EXPERIMENTAL)\n",
    "# Normilize the dataset\n",
    "if True:\n",
    "    # Check if multiindex_df is indeed a MultiIndex DataFrame\n",
    "    if isinstance(dataset.index, pd.MultiIndex):\n",
    "        # Normalize each patient's data\n",
    "        # This will apply z-score normalization per patient per feature\n",
    "        normalized_data = dataset.groupby(level=0).transform(lambda x: (x - x.mean()) / x.std())\n",
    "    \n",
    "        # Optionally fill NaN values if they are created by division by zero in cases where std is zero\n",
    "        normalized_data = normalized_data.fillna(0)\n",
    "    \n",
    "        # If you need to replace the old DataFrame with the new, normalized one\n",
    "        dataset.update(normalized_data)\n",
    "    else:\n",
    "        print(\"The dataframe does not have a MultiIndex as expected.\")\n",
    "\n",
    "    \n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    dataset = impute_linear_interpolation(dataset, col)\n",
    "    print(col)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98495b-d998-4edc-a705-ca09320c5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_indicators(df):\n",
    "    \"\"\"\n",
    "    This function adds a binary indicator for each feature in the dataframe.\n",
    "    Each indicator is 1 where the original data was NaN, and 0 otherwise.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The original dataframe with possible NaN values.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Augmented dataframe with additional NaN indicator features.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        df[column + '_nan'] = df[column].isna().astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf039a4-0611-405e-9d40-19cbe40df7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(X, y):\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "    print(index_0, index_1)\n",
    "\n",
    "    if len(index_0) > len(index_1):\n",
    "        index_0 = np.random.choice(index_0, size=len(index_1), replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([index_0, index_1])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    x_balanced = X.iloc[balanced_indices]\n",
    "    y_balanced = y.iloc[balanced_indices]\n",
    "\n",
    "    return x_balanced, y_balanced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd1c02-d34a-401d-a1f9-03e83b6ff1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training.\")\n",
    "\n",
    "X = dataset.drop('SepsisLabel', axis=1)\n",
    "y = dataset['SepsisLabel']\n",
    "\n",
    "X = add_nan_indicators(X)\n",
    "\n",
    "# Prepare data and add missingness indicators\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val = add_nan_indicators(X_train), add_nan_indicators(X_val)\n",
    "\n",
    "# X_train.fillna(0, inplace=True)\n",
    "# X_val.fillna(0, inplace=True)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = TimeSeriesTransformer(num_features=X_train.shape[1])  # Assuming features were doubled to account for indicators\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# To keep all the data focus on the minority class (sepsis = 1)\n",
    "\n",
    "# class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.values)\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCELoss(weight=class_weights[1])  # Focus more on the minority class\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 30\n",
    "best_auroc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            preds = output.squeeze()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(target.tolist())\n",
    "\n",
    "    # Convert prediction probabilities to binary predictions\n",
    "    threshold = 0.5  # This threshold can be adjusted\n",
    "    binary_preds = (output > threshold).int()\n",
    "    # binary_preds = [1 if prob >= threshold else 0 for prob in all_preds]\n",
    "\n",
    "    # Calculate scores\n",
    "    auroc = roc_auc_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, binary_preds)\n",
    "    recall = recall_score(all_targets, binary_preds)\n",
    "    f1 = f1_score(all_targets, binary_preds)\n",
    "    \n",
    "    if auroc > best_auroc:\n",
    "        best_auroc = auroc\n",
    "        # Save model and predictions for the best model based on AUROC\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Validation AUROC: {auroc}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-datascience] *",
   "language": "python",
   "name": "conda-env-.conda-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
