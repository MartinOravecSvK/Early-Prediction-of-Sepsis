{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f72d151-7591-4d00-a5d8-b985986461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28da82b0-0031-4338-ac71-e7b6526ab95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10823102-ef8b-4c0d-9fbc-d2d861a1499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeSeriesTransformer1(nn.Module):\n",
    "    def __init__(self, num_features, num_blocks=1, d_model=512, nhead=8, dim_feedforward=128, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        # self.embedding = nn.Linear(num_features, d_model)\n",
    "        # nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        print(src.shape)\n",
    "        # src = self.embedding(src)\n",
    "        # print(\"Post-embedding:\", src.min().item(), src.max().item())\n",
    "        \n",
    "        src = src.unsqueeze(1)  # Add a pseudo-sequence dimension\n",
    "        batch_size, seq_length, embedding_size = src.size()\n",
    "    \n",
    "        # Generate positional encodings dynamically based on sequence length and batch size\n",
    "        positional_encoding = torch.zeros(batch_size, seq_length, embedding_size).to(src.device)\n",
    "        position = torch.arange(seq_length, dtype=torch.float).unsqueeze(0).unsqueeze(-1).to(src.device)\n",
    "\n",
    "        print(\"Positional Encoding:\", positional_encoding.min().item(), positional_encoding.max().item())\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-math.log(10000.0) / embedding_size)).to(src.device)\n",
    "        positional_encoding[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        src += positional_encoding\n",
    "    \n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.output_layer(output)\n",
    "        print(\"Post-transformer encoder:\", output.min().item(), output.max().item())\n",
    "\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd8fac8c-7fb1-4e98-a498-5faa04baa6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_blocks=1, d_model=512, nhead=8, dim_feedforward=128, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def _generate_positional_encodings(self, max_seq_len, d_model):\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        if src.dim() == 2:  # If input has shape (batch_size, seq_length)\n",
    "            src = src.unsqueeze(-1)  # Add a dimension for num_features\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding.weight.size(1))\n",
    "        \n",
    "        batch_size, seq_length, num_features = src.size()\n",
    "        max_seq_len = 1000  # Maximum sequence length for positional encodings\n",
    "        if seq_length > max_seq_len:\n",
    "            raise ValueError(\"Input sequence length exceeds maximum sequence length for positional encodings.\")\n",
    "        \n",
    "        positional_encoding = self._generate_positional_encodings(seq_length, num_features)\n",
    "        src = src + positional_encoding[:, :seq_length, :].to(src.device)\n",
    "        \n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return torch.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45768cab-ba20-48d5-bfdd-d37ea7863a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b726056a-3f24-44eb-ac28-5f03223f2f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No random values\n"
     ]
    }
   ],
   "source": [
    "for yy in dataset[\"SepsisLabel\"]:\n",
    "    if yy > 0.0 and yy < 1.0:\n",
    "        print(yy)\n",
    "print(\"No random values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb84bc8-7790-436e-a7b6-308831910611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly interpolating:\n",
      "HR\n",
      "O2Sat\n",
      "SBP\n",
      "MAP\n",
      "DBP\n",
      "Resp\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# First lets experiment with only raw data \n",
    "# We have to however impute NaN values since Neural Networks can't (natively) handle them\n",
    "\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "\n",
    "# Feel free to omit this (EXPERIMENTAL)\n",
    "# Normilize the dataset\n",
    "if True:\n",
    "    # Check if multiindex_df is indeed a MultiIndex DataFrame\n",
    "    if isinstance(dataset.index, pd.MultiIndex):\n",
    "        # Exclude 'SepsisLabel' from normalization\n",
    "        features_to_normalize = dataset.columns.difference(['SepsisLabel'])\n",
    "\n",
    "        # Normalize each patient's data\n",
    "        # This will apply z-score normalization per patient per feature, excluding 'SepsisLabel'\n",
    "        normalized_data = dataset[features_to_normalize].groupby(level=0).transform(\n",
    "            lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "        # Optionally fill NaN values if they are created by division by zero in cases where std is zero\n",
    "        normalized_data = normalized_data.fillna(0)\n",
    "\n",
    "        # Merge normalized data with the 'SepsisLabel' column\n",
    "        dataset = pd.concat([normalized_data, dataset['SepsisLabel']], axis=1)\n",
    "    else:\n",
    "        print(\"The dataframe does not have a MultiIndex as expected.\")\n",
    "\n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    if col != 'SepsisLabel':  # Ensure we do not interpolate 'SepsisLabel'\n",
    "        dataset = impute_linear_interpolation(dataset, col)\n",
    "        print(col)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0ca3b92-b0e9-49b3-a4bd-a61919f22d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No random values\n"
     ]
    }
   ],
   "source": [
    "for yy in dataset[\"SepsisLabel\"]:\n",
    "    if yy > 0.0 and yy < 1.0:\n",
    "        print(yy)\n",
    "print(\"No random values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc98495b-d998-4edc-a705-ca09320c5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_indicators(df):\n",
    "    \"\"\"\n",
    "    This function adds a binary indicator for each feature in the dataframe.\n",
    "    Each indicator is 1 where the original data was NaN, and 0 otherwise.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The original dataframe with possible NaN values.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Augmented dataframe with additional NaN indicator features.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        df[column + '_nan'] = df[column].isna().astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf039a4-0611-405e-9d40-19cbe40df7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(X, y):\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "    print(index_0, index_1)\n",
    "\n",
    "    if len(index_0) > len(index_1):\n",
    "        index_0 = np.random.choice(index_0, size=len(index_1), replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([index_0, index_1])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    x_balanced = X.iloc[balanced_indices]\n",
    "    y_balanced = y.iloc[balanced_indices]\n",
    "\n",
    "    return x_balanced, y_balanced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7bd1c02-d34a-401d-a1f9-03e83b6ff1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for training.\n",
      "120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=120, out_features=512, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=120, out_features=512, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 60\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), target)\n\u001b[0;32m     62\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\datascience\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\datascience\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[64], line 33\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(src) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 33\u001b[0m     batch_size, seq_length, _ \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m     34\u001b[0m     max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Maximum sequence length for positional encodings\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seq_length \u001b[38;5;241m>\u001b[39m max_seq_len:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training.\")\n",
    "\n",
    "X = dataset.drop('SepsisLabel', axis=1)\n",
    "y = dataset['SepsisLabel']\n",
    "\n",
    "X = add_nan_indicators(X)\n",
    "\n",
    "# Prepare data and add missingness indicators\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val = add_nan_indicators(X_train), add_nan_indicators(X_val)\n",
    "\n",
    "# X_train.fillna(0, inplace=True)\n",
    "# X_val.fillna(0, inplace=True)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = TimeSeriesTransformer(num_features=X_train.shape[1])  # Assuming features were doubled to account for indicators\n",
    "print(X_train.shape[1])\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# To keep all the data focus on the minority class (sepsis = 1)\n",
    "\n",
    "# class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.values)\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCELoss(weight=class_weights[1])  # Focus more on the minority class\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 30\n",
    "best_auroc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            preds = output.squeeze()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(target.tolist())\n",
    "\n",
    "    # Convert prediction probabilities to binary predictions\n",
    "    threshold = 0.5  # This threshold can be adjusted\n",
    "    binary_preds = (output > threshold).int()\n",
    "    # binary_preds = [1 if prob >= threshold else 0 for prob in all_preds]\n",
    "\n",
    "    # Calculate scores\n",
    "    auroc = roc_auc_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, binary_preds)\n",
    "    recall = recall_score(all_targets, binary_preds)\n",
    "    f1 = f1_score(all_targets, binary_preds)\n",
    "    \n",
    "    if auroc > best_auroc:\n",
    "        best_auroc = auroc\n",
    "        # Save model and predictions for the best model based on AUROC\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Validation AUROC: {auroc}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bf675-cec7-4f69-9156-e2958fa66a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba16b9b-6446-4ded-aa49-63d6da282837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-datascience] *",
   "language": "python",
   "name": "conda-env-.conda-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
