{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f72d151-7591-4d00-a5d8-b985986461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28da82b0-0031-4338-ac71-e7b6526ab95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f71f3c06-6c85-4923-98e7-18855bec62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x.transpose(0, 1))\n",
    "        x = self.decoder(x.transpose(0, 1))\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "08007f84-8b79-4f55-950d-c21a3b7b6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x.transpose(0, 1))\n",
    "        x = self.decoder(x)\n",
    "        return x.squeeze(-1)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45768cab-ba20-48d5-bfdd-d37ea7863a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b726056a-3f24-44eb-ac28-5f03223f2f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1552210, 41)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb84bc8-7790-436e-a7b6-308831910611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly interpolating:\n",
      "HR\n",
      "O2Sat\n",
      "SBP\n",
      "MAP\n",
      "DBP\n",
      "Resp\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# First lets experiment with only raw data \n",
    "# We have to however impute NaN values since Neural Networks can't (natively) handle them\n",
    "\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "\n",
    "# Feel free to omit this (EXPERIMENTAL)\n",
    "# Normilize the dataset\n",
    "if True:\n",
    "    # Check if multiindex_df is indeed a MultiIndex DataFrame\n",
    "    if isinstance(dataset.index, pd.MultiIndex):\n",
    "        # Exclude 'SepsisLabel' from normalization\n",
    "        features_to_normalize = dataset.columns.difference(['SepsisLabel'])\n",
    "\n",
    "        # Normalize each patient's data\n",
    "        # This will apply z-score normalization per patient per feature, excluding 'SepsisLabel'\n",
    "        normalized_data = dataset[features_to_normalize].groupby(level=0).transform(\n",
    "            lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "        # Optionally fill NaN values if they are created by division by zero in cases where std is zero\n",
    "        normalized_data = normalized_data.fillna(0)\n",
    "\n",
    "        # Merge normalized data with the 'SepsisLabel' column\n",
    "        dataset = pd.concat([normalized_data, dataset['SepsisLabel']], axis=1)\n",
    "    else:\n",
    "        print(\"The dataframe does not have a MultiIndex as expected.\")\n",
    "\n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    if col != 'SepsisLabel':  # Ensure we do not interpolate 'SepsisLabel'\n",
    "        dataset = impute_linear_interpolation(dataset, col)\n",
    "        print(col)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "23f1c375-50f3-410b-9383-053786f17938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1552210, 41)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc98495b-d998-4edc-a705-ca09320c5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_indicators(df):\n",
    "    for column in df.columns:\n",
    "        df[column + '_nan'] = df[column].isna().astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf039a4-0611-405e-9d40-19cbe40df7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(X, y):\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "    print(index_0, index_1)\n",
    "\n",
    "    if len(index_0) > len(index_1):\n",
    "        index_0 = np.random.choice(index_0, size=len(index_1), replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([index_0, index_1])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    x_balanced = X.iloc[balanced_indices]\n",
    "    y_balanced = y.iloc[balanced_indices]\n",
    "\n",
    "    return x_balanced, y_balanced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a1f7eec6-1195-4a0e-a36d-c87b7ed63995",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TimeSeriesTransformer(input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "model = TimeSeriesTransformer(input_dim=X_train.shape[1])\n",
    "summary(model, input_size=(1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f4523-880b-4695-bc66-9b8e43a8d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2dfacbf6-610c-4827-8466-666feeaac34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(54, 80)\n",
      "(23, 80)\n",
      "(48, 80)\n",
      "(29, 80)\n",
      "(48, 80)\n",
      "(17, 80)\n",
      "(45, 80)\n",
      "(40, 80)\n",
      "(258, 80)\n",
      "(23, 80)\n",
      "(34, 80)\n",
      "(21, 80)\n"
     ]
    }
   ],
   "source": [
    "X = dataset.drop('SepsisLabel', axis=1)\n",
    "X = add_nan_indicators(X)\n",
    "y = dataset['SepsisLabel']\n",
    "\n",
    "for patient_id in X.index.get_level_values('patient_id').unique():\n",
    "    print(patient_id)\n",
    "    break\n",
    "\n",
    "a = 0\n",
    "for patient_id, patient_data in X.groupby(level='patient_id'):\n",
    "    print(patient_data.shape)\n",
    "    if a > 10:\n",
    "        break\n",
    "    a += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7bd1c02-d34a-401d-a1f9-03e83b6ff1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (embedding): Linear(in_features=120, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Prepare data and add missingness indicators\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val = add_nan_indicators(X_train), add_nan_indicators(X_val)\n",
    "\n",
    "# X_train.fillna(0, inplace=True)\n",
    "# X_val.fillna(0, inplace=True)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "# model = TimeSeriesTransformer(num_features=X_train.shape[1])  # Assuming features were doubled to account for indicators\n",
    "model = TimeSeriesTransformer(input_dim=X_train.shape[1])\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# To keep all the data focus on the minority class (sepsis = 1)\n",
    "\n",
    "# class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.values)\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCELoss(weight=class_weights[1])  # Focus more on the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e48bb6-270f-40b4-ac2a-d3196f3ecd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 30\n",
    "best_auroc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.squeeze()\n",
    "        \n",
    "        # output[:, 0][torch.isnan(output[:, 0])] = 0\n",
    "\n",
    "        # Forward fill NaN values in the rest of the tensor\n",
    "        # for i in range(1, output.size(1)):\n",
    "        #     output[:, i][torch.isnan(output[:, i])] = output[:, i-1][torch.isnan(output[:, i])]\n",
    "\n",
    "\n",
    "        # print(\"Output: \", output.shape)\n",
    "        # print(\"Target: \", target.shape)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            preds = output.squeeze()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(target.tolist())\n",
    "\n",
    "    # Convert prediction probabilities to binary predictions\n",
    "    threshold = 0.5  # This threshold can be adjusted\n",
    "    binary_preds = (output > threshold).int()\n",
    "    # binary_preds = [1 if prob >= threshold else 0 for prob in all_preds]\n",
    "\n",
    "    # Calculate scores\n",
    "    auroc = roc_auc_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, binary_preds)\n",
    "    recall = recall_score(all_targets, binary_preds)\n",
    "    f1 = f1_score(all_targets, binary_preds)\n",
    "    \n",
    "    if auroc > best_auroc:\n",
    "        best_auroc = auroc\n",
    "        # Save model and predictions for the best model based on AUROC\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Validation AUROC: {auroc}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-datascience] *",
   "language": "python",
   "name": "conda-env-.conda-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
