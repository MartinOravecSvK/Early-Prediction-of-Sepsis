{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f72d151-7591-4d00-a5d8-b985986461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "\n",
    "DATA_PATH = get_data.get_dataset_abspath()\n",
    "\n",
    "training_setA_path = DATA_PATH + 'training_setA'\n",
    "training_setB_path = DATA_PATH + 'training_setB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28da82b0-0031-4338-ac71-e7b6526ab95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Geniuses that worked on hypertools did not update certain package and thus it produces warnings (they break jupyter lab)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Comment out if you don't want to see all of the values being printed (i.e. default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# utils_path = os.path.join(current_dir, '..', 'utils')\n",
    "utils_path = os.path.join(current_dir, '../')\n",
    "utils_abs_path = os.path.abspath(utils_path)\n",
    "if utils_abs_path not in sys.path:\n",
    "    sys.path.append(utils_abs_path)\n",
    "\n",
    "import utils.get_data as get_data\n",
    "# from impute_methods import *\n",
    "from utils.impute_methods import impute_linear_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71f3c06-6c85-4923-98e7-18855bec62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x.transpose(0, 1))\n",
    "        x = self.decoder(x.transpose(0, 1))\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08007f84-8b79-4f55-950d-c21a3b7b6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x.transpose(0, 1))\n",
    "        x = self.decoder(x)\n",
    "        return x.squeeze(-1)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45768cab-ba20-48d5-bfdd-d37ea7863a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "dataset, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b726056a-3f24-44eb-ac28-5f03223f2f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1552210, 41)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb84bc8-7790-436e-a7b6-308831910611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly interpolating:\n",
      "HR\n",
      "O2Sat\n",
      "SBP\n",
      "MAP\n",
      "DBP\n",
      "Resp\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# First lets experiment with only raw data \n",
    "# We have to however impute NaN values since Neural Networks can't (natively) handle them\n",
    "\n",
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "\n",
    "# Feel free to omit this (EXPERIMENTAL)\n",
    "# Normilize the dataset\n",
    "if True:\n",
    "    # Check if multiindex_df is indeed a MultiIndex DataFrame\n",
    "    if isinstance(dataset.index, pd.MultiIndex):\n",
    "        # Exclude 'SepsisLabel' from normalization\n",
    "        features_to_normalize = dataset.columns.difference(['SepsisLabel'])\n",
    "\n",
    "        # Normalize each patient's data\n",
    "        # This will apply z-score normalization per patient per feature, excluding 'SepsisLabel'\n",
    "        normalized_data = dataset[features_to_normalize].groupby(level=0).transform(\n",
    "            lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "        # Optionally fill NaN values if they are created by division by zero in cases where std is zero\n",
    "        normalized_data = normalized_data.fillna(0)\n",
    "\n",
    "        # Merge normalized data with the 'SepsisLabel' column\n",
    "        dataset = pd.concat([normalized_data, dataset['SepsisLabel']], axis=1)\n",
    "    else:\n",
    "        print(\"The dataframe does not have a MultiIndex as expected.\")\n",
    "\n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    if col != 'SepsisLabel':  # Ensure we do not interpolate 'SepsisLabel'\n",
    "        dataset = impute_linear_interpolation(dataset, col)\n",
    "        print(col)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f1c375-50f3-410b-9383-053786f17938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1552210, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc98495b-d998-4edc-a705-ca09320c5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_indicators(df):\n",
    "    for column in df.columns:\n",
    "        df[column + '_nan'] = df[column].isna().astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf039a4-0611-405e-9d40-19cbe40df7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(X, y):\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "    print(index_0, index_1)\n",
    "\n",
    "    if len(index_0) > len(index_1):\n",
    "        index_0 = np.random.choice(index_0, size=len(index_1), replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([index_0, index_1])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    x_balanced = X.iloc[balanced_indices]\n",
    "    y_balanced = y.iloc[balanced_indices]\n",
    "\n",
    "    return x_balanced, y_balanced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a1f7eec6-1195-4a0e-a36d-c87b7ed63995",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TimeSeriesTransformer(input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "model = TimeSeriesTransformer(input_dim=X_train.shape[1])\n",
    "summary(model, input_size=(1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119f4523-880b-4695-bc66-9b8e43a8d2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dfacbf6-610c-4827-8466-666feeaac34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(54, 80)\n",
      "(23, 80)\n",
      "(48, 80)\n",
      "(29, 80)\n",
      "(48, 80)\n",
      "(17, 80)\n",
      "(45, 80)\n",
      "(40, 80)\n",
      "(258, 80)\n",
      "(23, 80)\n",
      "(34, 80)\n",
      "(21, 80)\n"
     ]
    }
   ],
   "source": [
    "X = dataset.drop('SepsisLabel', axis=1)\n",
    "X = add_nan_indicators(X)\n",
    "y = dataset['SepsisLabel']\n",
    "\n",
    "for patient_id in X.index.get_level_values('patient_id').unique():\n",
    "    print(patient_id)\n",
    "    break\n",
    "\n",
    "a = 0\n",
    "for patient_id, patient_data in X.groupby(level='patient_id'):\n",
    "    print(patient_data.shape)\n",
    "    if a > 10:\n",
    "        break\n",
    "    a += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e78a8c1-8850-4904-89f1-4fbb3ecae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_patient_data(patient_data, max_length):\n",
    "    # Standardizing the data\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(patient_data)\n",
    "    # Padding\n",
    "    padded_features = np.zeros((max_length, features.shape[1]))\n",
    "    sequence_length = min(max_length, features.shape[0])\n",
    "    padded_features[:sequence_length] = features[:sequence_length]\n",
    "    return torch.tensor(padded_features, dtype=torch.float32), sequence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcc3a0b2-00d7-4b06-aa86-b40c7127e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(outputs, targets, seq_lengths):\n",
    "    mask = torch.arange(outputs.size(0)).expand(len(seq_lengths), outputs.size(0)) < torch.tensor(seq_lengths).unsqueeze(1)\n",
    "    mask = mask.to(outputs.device)\n",
    "    outputs = outputs[mask]\n",
    "    targets = torch.cat([targets[i][:l] for i, l in enumerate(seq_lengths)])\n",
    "    return criterion(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d25a30-8c8e-4850-a7aa-c481ed78c384",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSepsisLabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m max_length \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X = df.drop('SepsisLabel', axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "max_length = X.groupby('patient_id').size().max()\n",
    "print(\"Max length (inputs will be padded to): \", max_length)\n",
    "\n",
    "patient_ids = X.index.get_level_values('patient_id').unique()\n",
    "for patient_id in patient_ids:\n",
    "    max_length_patient = X.loc[patient_id].shape[0]\n",
    "    if max_length_patient == max_length:\n",
    "        print(patient_id, max_length_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "693ed0be-7890-4b1b-ae48-6fb992fb33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, patient_ids, labels, X, y):\n",
    "        self.patient_ids = patient_ids\n",
    "        self.labels = labels\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pid = self.patient_ids[index]\n",
    "        patient_data = self.X.loc[pid]\n",
    "        X_train, seq_length = prepare_patient_data(patient_data, max_length)\n",
    "        y_train = self.y.loc[pid].values[:seq_length]\n",
    "        return X_train, torch.tensor(y_train, dtype=torch.float32), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd86b5c-549a-4f9d-aed4-ec7e6a9a19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeing if there are still any nan values or +/- infinities\n",
      "Data contains NaN or infinite values. Handling...\n",
      "Max length (inputs will be padded to):  336\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "df = dataset\n",
    "X = df.drop('SepsisLabel', axis=1)\n",
    "X = add_nan_indicators(X)\n",
    "y = df['SepsisLabel']\n",
    "\n",
    "print(\"Seeing if there are still any nan values or +/- infinities\")\n",
    "# Just trying to fix some errors I got only on a GPU\n",
    "if X.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "    print(\"Data contains NaN or infinite values. Handling...\")\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.fillna(method='ffill', inplace=True) \n",
    "\n",
    "# Ensure no NaNs or infinities in the target variable as well\n",
    "if y.isin([np.nan, np.inf, -np.inf]).any():\n",
    "    print(\"Target contains NaN or infinite values. Handling...\")\n",
    "    y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "# Yes it's really high, 336, consider making it larger to accommodate actual test set\n",
    "max_length = X.groupby('patient_id').size().max()\n",
    "print(\"Max length (inputs will be padded to): \", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1242234-20ed-4fed-a02c-4e75e704e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = X.index.get_level_values('patient_id').unique()\n",
    "train_ids, val_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = X.shape[1]\n",
    "model = TransformerTimeSeries(input_dim=input_dim)\n",
    "model.to(device)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y.to_numpy())\n",
    "class_weights_tensor = torch.tensor(class_weights[1], dtype=torch.float)  # Weight for the positive class\n",
    "\n",
    "# Initialize BCEWithLogitsLoss with pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "dataloader = DataLoader(PatientDataset(train_ids, y, X, y), batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "print(\"Started Training\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    train_preds, train_targets = [], []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for X_batch, y_batch, seq_lengths in dataloader:\n",
    "        try:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = compute_loss(outputs, y_batch, seq_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted_labels = (torch.sigmoid(outputs) > 0.5).int()\n",
    "            # Handle dynamic sequence length for accuracy calculation\n",
    "            for i, length in enumerate(seq_lengths):\n",
    "                train_correct += (predicted_labels[i][:length] == y_batch[i][:length]).sum().item()\n",
    "                train_total += length\n",
    "            \n",
    "            # train_loss += loss.item()\n",
    "            # valid_outputs = [outputs[:seq_lengths[i], i] for i in range(len(X_batch))]\n",
    "            # valid_labels = [y_batch[i, :seq_lengths[i]] for i in range(len(X_batch))]\n",
    "            # predicted_labels = [torch.round(torch.sigmoid(vo)) for vo in valid_outputs]\n",
    "            \n",
    "            # # Update metrics\n",
    "            # for i in range(len(predicted_labels)):\n",
    "            #     train_correct += (predicted_labels[i] == valid_labels[i]).sum().item()\n",
    "            #     train_total += seq_lengths[i]\n",
    "            #     train_preds.extend(predicted_labels[i].tolist())\n",
    "            #     train_targets.extend(valid_labels[i].tolist())\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing patient ID {patient_id}: {e}\")\n",
    "        \n",
    "        # Progress and time estimation\n",
    "        end_batch = time.time()\n",
    "        elapsed_time = end_batch - start_time\n",
    "        batches_done = i + 1\n",
    "        total_batches = len(train_ids)\n",
    "        time_per_batch = elapsed_time / batches_done\n",
    "        estimated_time_remaining = (total_batches - batches_done) * time_per_batch\n",
    "        \n",
    "        print(f'Processed {batches_done}/{total_batches} patients ({100.0 * batches_done / total_batches:.2f}%), ' +\n",
    "              f'Estimated time remaining: {estimated_time_remaining / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_precision = precision_score(train_targets, train_preds)\n",
    "    train_recall = recall_score(train_targets, train_preds)\n",
    "    train_f1 = f1_score(train_targets, train_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Avg Training Loss: {train_loss / len(train_ids)}, Training Metrics: Acc: {train_accuracy}, Precision: {train_precision}, Recall: {train_recall}, F1: {train_f1}')\n",
    "\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    val_preds, val_targets = [], []\n",
    "    start_val_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, patient_id in enumerate(val_ids):\n",
    "            start_batch = time.time()\n",
    "\n",
    "            try:\n",
    "                patient_data = X.loc[patient_id]\n",
    "                X_val, sequence_length = prepare_patient_data(patient_data, max_length)\n",
    "                y_val = torch.tensor(y.loc[patient_id].values, dtype=torch.float32)\n",
    "\n",
    "                val_outputs = model(X_val.unsqueeze(0))\n",
    "                v_loss = criterion(val_outputs[:sequence_length].squeeze(), y_val[:sequence_length])\n",
    "                val_loss += v_loss.item()\n",
    "\n",
    "                val_predicted_labels = torch.round(torch.sigmoid(val_outputs[:sequence_length].squeeze()))\n",
    "                val_correct += (val_predicted_labels == y_val[:sequence_length]).sum().item()\n",
    "                val_total += sequence_length\n",
    "\n",
    "                val_preds.extend(val_predicted_labels.tolist())\n",
    "                val_targets.extend(y_val[:sequence_length].tolist())\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing patient ID {patient_id}: {e}\")\n",
    "\n",
    "            # Progress and time estimation for validation\n",
    "            end_batch = time.time()\n",
    "            elapsed_time = end_batch - start_val_time\n",
    "            batches_done = i + 1\n",
    "            total_batches = len(val_ids)\n",
    "            time_per_batch = elapsed_time / batches_done\n",
    "            estimated_time_remaining = (total_batches - batches_done) * time_per_batch\n",
    "            \n",
    "            print(f'Validation: Processed {batches_done}/{total_batches} patients ({100.0 * batches_done / total_batches:.2f}%), ' +\n",
    "                  f'Estimated time remaining: {estimated_time_remaining / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_precision = precision_score(val_targets, val_preds)\n",
    "    val_recall = recall_score(val_targets, val_preds)\n",
    "    val_f1 = f1_score(val_targets, val_preds)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}, Avg Training Loss: {train_loss / len(train_ids)}, Training Metrics: Acc: {train_accuracy}, Precision: {train_precision}, Recall: {train_recall}, F1: {train_f1}')\n",
    "    print(f'Avg Validation Loss: {val_loss / len(val_ids)}, Validation Metrics: Acc: {val_accuracy}, Precision: {val_precision}, Recall: {val_recall}, F1: {val_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ee60a-8631-44bb-80fa-bc7ac44381bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "df = dataset\n",
    "X = df.drop('SepsisLabel', axis=1)\n",
    "X = add_nan_indicators(X)\n",
    "y = df['SepsisLabel']\n",
    "\n",
    "print(\"Seeing if there are still any nan values or +/- infinities\")\n",
    "# Just trying to fix some errors I got only on a GPU\n",
    "if X.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "    print(\"Data contains NaN or infinite values. Handling...\")\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.fillna(method='ffill', inplace=True) \n",
    "\n",
    "# Ensure no NaNs or infinities in the target variable as well\n",
    "if y.isin([np.nan, np.inf, -np.inf]).any():\n",
    "    print(\"Target contains NaN or infinite values. Handling...\")\n",
    "    y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "# Yes it's really high, 336, consider making it larger to accommodate actual test set\n",
    "max_length = X.groupby('patient_id').size().max()\n",
    "print(\"Max length (inputs will be padded to): \", max_length)\n",
    "\n",
    "patient_ids = X.index.get_level_values('patient_id').unique()\n",
    "train_ids, val_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = X.shape[1]\n",
    "model = TransformerTimeSeries(input_dim=input_dim)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y.to_numpy())\n",
    "class_weights_tensor = torch.tensor(class_weights[1], dtype=torch.float)  # Weight for the positive class\n",
    "\n",
    "# Initialize BCEWithLogitsLoss with pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    train_preds, train_targets = [], []\n",
    "    \n",
    "    start_time = time.time()  # Start time for the epoch\n",
    "    \n",
    "    for i, patient_id in enumerate(train_ids):\n",
    "        start_batch = time.time()  # Start time for the batch\n",
    "        \n",
    "        try:\n",
    "            patient_data = X.loc[patient_id]\n",
    "            X_train, sequence_length = prepare_patient_data(patient_data, max_length)\n",
    "            y_train = torch.tensor(y.loc[patient_id].values, dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train.unsqueeze(0))\n",
    "            loss = criterion(outputs[:sequence_length].squeeze(), y_train[:sequence_length])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted_labels = torch.round(torch.sigmoid(outputs[:sequence_length].squeeze()))\n",
    "            train_correct += (predicted_labels == y_train[:sequence_length]).sum().item()\n",
    "            train_total += sequence_length\n",
    "\n",
    "            train_preds.extend(predicted_labels.tolist())\n",
    "            train_targets.extend(y_train[:sequence_length].tolist())\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing patient ID {patient_id}: {e}\")\n",
    "        \n",
    "        # Progress and time estimation\n",
    "        end_batch = time.time()\n",
    "        elapsed_time = end_batch - start_time\n",
    "        batches_done = i + 1\n",
    "        total_batches = len(train_ids)\n",
    "        time_per_batch = elapsed_time / batches_done\n",
    "        estimated_time_remaining = (total_batches - batches_done) * time_per_batch\n",
    "        \n",
    "        print(f'Processed {batches_done}/{total_batches} patients ({100.0 * batches_done / total_batches:.2f}%), ' +\n",
    "              f'Estimated time remaining: {estimated_time_remaining / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_precision = precision_score(train_targets, train_preds)\n",
    "    train_recall = recall_score(train_targets, train_preds)\n",
    "    train_f1 = f1_score(train_targets, train_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Avg Training Loss: {train_loss / len(train_ids)}, Training Metrics: Acc: {train_accuracy}, Precision: {train_precision}, Recall: {train_recall}, F1: {train_f1}')\n",
    "\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    val_preds, val_targets = [], []\n",
    "    start_val_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, patient_id in enumerate(val_ids):\n",
    "            start_batch = time.time()\n",
    "\n",
    "            try:\n",
    "                patient_data = X.loc[patient_id]\n",
    "                X_val, sequence_length = prepare_patient_data(patient_data, max_length)\n",
    "                y_val = torch.tensor(y.loc[patient_id].values, dtype=torch.float32)\n",
    "\n",
    "                val_outputs = model(X_val.unsqueeze(0))\n",
    "                v_loss = criterion(val_outputs[:sequence_length].squeeze(), y_val[:sequence_length])\n",
    "                val_loss += v_loss.item()\n",
    "\n",
    "                val_predicted_labels = torch.round(torch.sigmoid(val_outputs[:sequence_length].squeeze()))\n",
    "                val_correct += (val_predicted_labels == y_val[:sequence_length]).sum().item()\n",
    "                val_total += sequence_length\n",
    "\n",
    "                val_preds.extend(val_predicted_labels.tolist())\n",
    "                val_targets.extend(y_val[:sequence_length].tolist())\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing patient ID {patient_id}: {e}\")\n",
    "\n",
    "            # Progress and time estimation for validation\n",
    "            end_batch = time.time()\n",
    "            elapsed_time = end_batch - start_val_time\n",
    "            batches_done = i + 1\n",
    "            total_batches = len(val_ids)\n",
    "            time_per_batch = elapsed_time / batches_done\n",
    "            estimated_time_remaining = (total_batches - batches_done) * time_per_batch\n",
    "            \n",
    "            print(f'Validation: Processed {batches_done}/{total_batches} patients ({100.0 * batches_done / total_batches:.2f}%), ' +\n",
    "                  f'Estimated time remaining: {estimated_time_remaining / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_precision = precision_score(val_targets, val_preds)\n",
    "    val_recall = recall_score(val_targets, val_preds)\n",
    "    val_f1 = f1_score(val_targets, val_preds)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}, Avg Training Loss: {train_loss / len(train_ids)}, Training Metrics: Acc: {train_accuracy}, Precision: {train_precision}, Recall: {train_recall}, F1: {train_f1}')\n",
    "    print(f'Avg Validation Loss: {val_loss / len(val_ids)}, Validation Metrics: Acc: {val_accuracy}, Precision: {val_precision}, Recall: {val_recall}, F1: {val_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4363c48-74d5-40aa-83e3-2f0f2b8bb610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-test",
   "language": "python",
   "name": "cuda-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
