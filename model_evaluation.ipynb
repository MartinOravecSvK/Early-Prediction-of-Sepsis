{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd33e6e-3aa3-476a-9878-f08ee7a515ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same number of sepsis label = 0 as there are sepsis label =  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5015e5-c4a1-4c0c-947f-ab58cf1c7722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\anaconda3\\envs\\lmm\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xgboost\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e3752d-b2a6-4c2b-8b2a-c0a88b364e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.get_data as get_data\n",
    "from utils.impute_methods import impute_linear_interpolation\n",
    "from utils.feature_engineering import preprecess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a717d42-9ff5-4123-9a35-0630b3021422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232f3be1-0d5f-4005-8e17-f7188a799949",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af995d2-f7ed-4528-817f-f620919d12ee",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0ba08b-0faf-470d-ba70-97802a80bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, patient_ids, X, y, max_length, device):\n",
    "        self.patient_ids = patient_ids\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pid = self.patient_ids[index]\n",
    "        patient_data = self.X.loc[pid]\n",
    "        X_train, seq_length = prepare_patient_data(patient_data, self.max_length)\n",
    "        y_train = torch.tensor(self.y.loc[pid].values, dtype=torch.float32)\n",
    "\n",
    "        # Ensure y_train is appropriately padded or trimmed to match X_train's length\n",
    "        if len(y_train) > self.max_length:\n",
    "            y_train = y_train[:self.max_length]\n",
    "        elif len(y_train) < self.max_length:\n",
    "            y_train = pad(torch.tensor(y_train, dtype=torch.float32), (0, self.max_length - len(y_train)), value=0)\n",
    "        \n",
    "        return X_train, y_train, len(y_train)\n",
    "        # return X_train, y_train, seq_length\n",
    "\n",
    "\n",
    "def prepare_patient_data(patient_data, max_length): \n",
    "        # Standardizing the data\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(patient_data)\n",
    "        # Padding\n",
    "        padded_features = np.zeros((max_length, features.shape[1]))\n",
    "        sequence_length = min(max_length, features.shape[0])\n",
    "        padded_features[:sequence_length] = features[:sequence_length]\n",
    "        return torch.tensor(padded_features, dtype=torch.float32), sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55eda4c-a0b8-4b16-84ac-162981059bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n"
     ]
    }
   ],
   "source": [
    "dataset_raw, patient_id_map = get_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9aceb3-9484-4563-a4aa-3390c590d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepsis_groups = dataset_raw.groupby(level=\"patient_id\")[\"SepsisLabel\"].max()\n",
    "patients_sepsis = sepsis_groups[sepsis_groups == 1].index\n",
    "patients_no_sepsis = sepsis_groups[sepsis_groups == 0].index\n",
    "patients_no_sepsis_sample = sepsis_groups.loc[patients_no_sepsis].sample(n=len(patients_sepsis)).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be92a495-b259-4647-9077-9ce35c5e2405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SepsisLabel\n",
       "0    2932\n",
       "1    2932\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = patients_no_sepsis_sample.append(patients_sepsis)\n",
    "\n",
    "# Get the equal number of sepsis and non-sepsis patients and shuffle\n",
    "dataset = dataset_raw.loc[indices].sample(frac=1)\n",
    "\n",
    "# Check that there is equal number of sepsis and non-sepsis patients\n",
    "dataset.groupby(level=\"patient_id\")[\"SepsisLabel\"].max().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc1e106-e963-4bd3-aca5-4617ffd635f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_proportion = 0.2\n",
    "# num_patients = len(dataset.index.levels[0])\n",
    "# subset_size = int(subset_proportion * num_patients)\n",
    "# print(f\"Test set size: {subset_size} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a212ac-a163-4765-9225-4b4b61fcbf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279461, 41)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c59afe0-1028-440d-9251-7ce9ec804509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly interpolating:\n",
      "HR\n",
      "O2Sat\n",
      "SBP\n",
      "MAP\n",
      "DBP\n",
      "Resp\n"
     ]
    }
   ],
   "source": [
    "columns_to_linearly_interpolate = [\n",
    "    'HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Resp'\n",
    "]\n",
    "\n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    if col != 'SepsisLabel':  # Ensure we do not interpolate 'SepsisLabel'\n",
    "        dataset = impute_linear_interpolation(dataset, col)\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1610e1d0-a98c-4307-9ee0-4a00a0c15735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_indicators(df):\n",
    "    for column in df.columns:\n",
    "        df[column + '_nan'] = df[column].isna().astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c98273-bce0-4de5-926c-247a64f8ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = add_nan_indicators(dataset)\n",
    "\n",
    "# Engineered\n",
    "XX, y_engineered = preprecess_data(X)\n",
    "new_feature_names = [f\"new_feature_{i}\" for i in range(XX.shape[1])]\n",
    "XX_df = pd.DataFrame(XX, columns=new_feature_names, index=X.index)\n",
    "X_engineered = pd.concat([X, XX_df], axis=1)\n",
    "\n",
    "# Base\n",
    "X = dataset.drop('SepsisLabel', axis=1) \n",
    "y = dataset['SepsisLabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6f058b-33a6-434b-bb16-b7179107aa14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains NaN or infinite values. Handling...\n",
      "Data contains NaN or infinite values. Handling...\n"
     ]
    }
   ],
   "source": [
    "if X_engineered.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "    print(\"Data contains NaN or infinite values. Handling...\")\n",
    "    # Replace infinite values with NaN so they can be filled too\n",
    "    X_engineered.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # First apply forward fill\n",
    "    X_engineered.fillna(method='ffill', inplace=True)\n",
    "    # Then apply backward fill for any remaining NaNs\n",
    "    X_engineered.fillna(method='bfill', inplace=True)\n",
    "\n",
    "if X.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "    print(\"Data contains NaN or infinite values. Handling...\")\n",
    "    # Replace infinite values with NaN so they can be filled too\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # First apply forward fill\n",
    "    X.fillna(method='ffill', inplace=True)\n",
    "    # Then apply backward fill for any remaining NaNs\n",
    "    X.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Ensure no NaNs or infinities in the target variable as well\n",
    "if y.isin([np.nan, np.inf, -np.inf]).any():\n",
    "    print(\"Target contains NaN or infinite values. Handling...\")\n",
    "    y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf738edc-e01b-4980-81d0-0d7a71775e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length (inputs will be padded to):  336\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum sequence length for padding\n",
    "# Yes it's really high, 336, consider making it larger to accommodate actual test set\n",
    "max_length = X.groupby('patient_id').size().max()\n",
    "print(\"Max length (inputs will be padded to): \", max_length)\n",
    "\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_patient_ids = X_test.index.get_level_values('patient_id').unique()\n",
    "\n",
    "_, X_engineered_test, _, y_engineered_test = train_test_split(X_engineered, y, test_size=0.2, random_state=42)\n",
    "X_engineered_patient_ids = X_engineered_test.index.get_level_values('patient_id').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86409435-5388-4306-b5ba-cc567febe47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55893, 81)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3149cfb1-8a28-4f54-8f8e-9c3061023034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55893, 301)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_engineered_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbc33019-2ff8-4f93-bfc9-4cf9f481c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55893,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca3b6acd-94e3-4c0f-8455-4febfc64d879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55893,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_engineered_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905f992-d578-45ca-952a-5d0eb377f89d",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de194839-2974-40b4-aa60-8f521f57c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformers\n",
    "transformers_dir = os.getcwd() + \"/models/transformer\"\n",
    "transformer_paths = [os.path.join(transformers_dir, path) for path in os.listdir(transformers_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30024663-1468-4a00-aed1-29d9b19ab8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xgboost\n",
    "xgb_path = \"models/saved/xgboost_model1.mdl\"\n",
    "xgb = xgboost.XGBClassifier()\n",
    "xgb.load_model(xgb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1e3d33a-65d0-4855-9d5d-c5f1885d5cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 176, got 81",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xgb_pred \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\xgboost\\sklearn.py:1553\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m   1545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1546\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1550\u001b[0m     iteration_range: Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1551\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[1;32m-> 1553\u001b[0m         class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[0;32m   1561\u001b[0m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\xgboost\\sklearn.py:1168\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1168\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1177\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\xgboost\\core.py:2428\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2425\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2426\u001b[0m         )\n\u001b[0;32m   2427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m-> 2428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2429\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2430\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2431\u001b[0m         )\n\u001b[0;32m   2433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[0;32m   2434\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 176, got 81"
     ]
    }
   ],
   "source": [
    "xgb_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95f97cac-9118-4cf5-afaa-e39692612426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_predictions(model):\n",
    "        preds = []\n",
    "        for patient_index in X_patient_ids:\n",
    "            data = X_test[X_test.index.get_level_values(0) == patient_index]\n",
    "            pred = model(torch.tensor(data.astype(\"float32\").values).to(device))\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "\n",
    "\n",
    "def get_engineered_transformer_predictions(model):\n",
    "        preds = []\n",
    "        for patient_index in X_engineered_patient_ids:\n",
    "            data = X_engineered_test[X_engineered_test.index.get_level_values(0) == patient_index]\n",
    "            pred = model(torch.tensor(data.astype(\"float32\").values).to(device))\n",
    "            preds.append(pred)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb83cd7c-5eea-4f77-992d-e8c3fd7e8e04",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.00 GiB is allocated by PyTorch, and 46.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengineered\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m path:\n\u001b[1;32m----> 7\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mget_engineered_transformer_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     preds \u001b[38;5;241m=\u001b[39m get_transformer_predictions(model)\n",
      "Cell \u001b[1;32mIn[70], line 14\u001b[0m, in \u001b[0;36mget_engineered_transformer_predictions\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patient_index \u001b[38;5;129;01min\u001b[39;00m X_engineered_patient_ids:\n\u001b[0;32m     13\u001b[0m     data \u001b[38;5;241m=\u001b[39m X_engineered_test[X_engineered_test\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m patient_index]\n\u001b[1;32m---> 14\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Year3\\ADS\\ADS_COURSEWORK\\Early-Prediction-of-Sepsis\\models\\architecture\\Transformer.py:35\u001b[0m, in \u001b[0;36mTransformerTimeSeries.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(x)\n\u001b[1;32m---> 35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[1;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:730\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 730\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lmm\\lib\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.00 GiB is allocated by PyTorch, and 46.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "all_transformers_preds = []\n",
    "\n",
    "for path in transformer_paths[:1]:\n",
    "    model = torch.load(path)\n",
    "\n",
    "    if \"engineered\" in path:\n",
    "        preds = get_engineered_transformer_predictions(model)\n",
    "    else:\n",
    "        preds = get_transformer_predictions(model)\n",
    "\n",
    "    all_transformers_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6ea30-ad70-4ef7-bc23-b068718de40b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
